21:C 28 May 2023 05:49:08.838 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
21:C 28 May 2023 05:49:08.838 # Redis version=6.2.7, bits=64, commit=00000000, modified=0, pid=21, just started
21:C 28 May 2023 05:49:08.838 # Configuration loaded
21:M 28 May 2023 05:49:08.839 * monotonic clock: POSIX clock_gettime
21:M 28 May 2023 05:49:08.841 # A key '__redis__compare_helper' was added to Lua globals which is not on the globals allow list nor listed on the deny list.
                _._                                                  
           _.-``__ ''-._                                             
      _.-``    `.  `_.  ''-._           Redis 6.2.7 (00000000/0) 64 bit
  .-`` .-```.  ```\/    _.,_ ''-._                                  
 (    '      ,       .-`  | `,    )     Running in standalone mode
 |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379
 |    `-._   `._    /     _.-'    |     PID: 21
  `-._    `-._  `-./  _.-'    _.-'                                   
 |`-._`-._    `-.__.-'    _.-'_.-'|                                  
 |    `-._`-._        _.-'_.-'    |           https://redis.io       
  `-._    `-._`-.__.-'_.-'    _.-'                                   
 |`-._`-._    `-.__.-'    _.-'_.-'|                                  
 |    `-._`-._        _.-'_.-'    |                                  
  `-._    `-._`-.__.-'_.-'    _.-'                                   
      `-._    `-.__.-'    _.-'                                       
          `-._        _.-'                                           
              `-.__.-'                                               

21:M 28 May 2023 05:49:08.841 # Server initialized
21:M 28 May 2023 05:49:08.841 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
21:M 28 May 2023 05:49:08.842 * Ready to accept connections
/usr/local/lib/python3.6/dist-packages/paramiko/transport.py:32: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography. The next release of cryptography will remove support for Python 3.6.
  from cryptography.hazmat.backends import default_backend
/usr/local/lib/python3.6/dist-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/
  warnings.warn(warning, PythonDeprecationWarning)
2023-05-28 05:49:17,025 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training
2023-05-28 05:49:17,037 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)
/usr/local/lib/python3.6/dist-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/
  warnings.warn(warning, PythonDeprecationWarning)
2023-05-28 05:49:17,336 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)
2023-05-28 05:49:17,362 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)
2023-05-28 05:49:17,388 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)
2023-05-28 05:49:17,408 sagemaker-containers INFO     Invoking user script

Training Env:

{
    "additional_framework_parameters": {
        "sagemaker_estimator": "RLEstimator"
    },
    "channel_input_dirs": {},
    "current_host": "algo-1-a8iza",
    "framework_module": "sagemaker_tensorflow_container.training:main",
    "hosts": [
        "algo-1-a8iza"
    ],
    "hyperparameters": {
        "s3_bucket": "bucket",
        "s3_prefix": "rl-deepracer-sagemaker",
        "aws_region": "us-east-1",
        "model_metadata_s3_key": "s3://bucket/custom_files/model_metadata.json",
        "RLCOACH_PRESET": "deepracer",
        "batch_size": 64,
        "beta_entropy": 0.01,
        "discount_factor": 0.995,
        "e_greedy_value": 0.05,
        "epsilon_steps": 10000,
        "exploration_type": "categorical",
        "loss_type": "huber",
        "lr": 0.0003,
        "num_episodes_between_training": 20,
        "num_epochs": 10,
        "stack_size": 1,
        "term_cond_avg_score": 400.0,
        "term_cond_max_episodes": 3600,
        "sac_alpha": 0.2
    },
    "input_config_dir": "/opt/ml/input/config",
    "input_data_config": {},
    "input_dir": "/opt/ml/input",
    "is_master": true,
    "job_name": "rl-deepracer-sagemaker",
    "log_level": 20,
    "master_hostname": "algo-1-a8iza",
    "model_dir": "/opt/ml/model",
    "module_dir": "s3://bucket/rl-deepracer-sagemaker/source/sourcedir.tar.gz",
    "module_name": "training_worker",
    "network_interface_name": "eth0",
    "num_cpus": 8,
    "num_gpus": 0,
    "output_data_dir": "/opt/ml/output/data",
    "output_dir": "/opt/ml/output",
    "output_intermediate_dir": "/opt/ml/output/intermediate",
    "resource_config": {
        "current_host": "algo-1-a8iza",
        "hosts": [
            "algo-1-a8iza"
        ]
    },
    "user_entry_point": "training_worker.py"
}

Environment variables:

SM_HOSTS=["algo-1-a8iza"]
SM_NETWORK_INTERFACE_NAME=eth0
SM_HPS={"RLCOACH_PRESET":"deepracer","aws_region":"us-east-1","batch_size":64,"beta_entropy":0.01,"discount_factor":0.995,"e_greedy_value":0.05,"epsilon_steps":10000,"exploration_type":"categorical","loss_type":"huber","lr":0.0003,"model_metadata_s3_key":"s3://bucket/custom_files/model_metadata.json","num_episodes_between_training":20,"num_epochs":10,"s3_bucket":"bucket","s3_prefix":"rl-deepracer-sagemaker","sac_alpha":0.2,"stack_size":1,"term_cond_avg_score":400.0,"term_cond_max_episodes":3600}
SM_USER_ENTRY_POINT=training_worker.py
SM_FRAMEWORK_PARAMS={"sagemaker_estimator":"RLEstimator"}
SM_RESOURCE_CONFIG={"current_host":"algo-1-a8iza","hosts":["algo-1-a8iza"]}
SM_INPUT_DATA_CONFIG={}
SM_OUTPUT_DATA_DIR=/opt/ml/output/data
SM_CHANNELS=[]
SM_CURRENT_HOST=algo-1-a8iza
SM_MODULE_NAME=training_worker
SM_LOG_LEVEL=20
SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main
SM_INPUT_DIR=/opt/ml/input
SM_INPUT_CONFIG_DIR=/opt/ml/input/config
SM_OUTPUT_DIR=/opt/ml/output
SM_NUM_CPUS=8
SM_NUM_GPUS=0
SM_MODEL_DIR=/opt/ml/model
SM_MODULE_DIR=s3://bucket/rl-deepracer-sagemaker/source/sourcedir.tar.gz
SM_TRAINING_ENV={"additional_framework_parameters":{"sagemaker_estimator":"RLEstimator"},"channel_input_dirs":{},"current_host":"algo-1-a8iza","framework_module":"sagemaker_tensorflow_container.training:main","hosts":["algo-1-a8iza"],"hyperparameters":{"RLCOACH_PRESET":"deepracer","aws_region":"us-east-1","batch_size":64,"beta_entropy":0.01,"discount_factor":0.995,"e_greedy_value":0.05,"epsilon_steps":10000,"exploration_type":"categorical","loss_type":"huber","lr":0.0003,"model_metadata_s3_key":"s3://bucket/custom_files/model_metadata.json","num_episodes_between_training":20,"num_epochs":10,"s3_bucket":"bucket","s3_prefix":"rl-deepracer-sagemaker","sac_alpha":0.2,"stack_size":1,"term_cond_avg_score":400.0,"term_cond_max_episodes":3600},"input_config_dir":"/opt/ml/input/config","input_data_config":{},"input_dir":"/opt/ml/input","is_master":true,"job_name":"rl-deepracer-sagemaker","log_level":20,"master_hostname":"algo-1-a8iza","model_dir":"/opt/ml/model","module_dir":"s3://bucket/rl-deepracer-sagemaker/source/sourcedir.tar.gz","module_name":"training_worker","network_interface_name":"eth0","num_cpus":8,"num_gpus":0,"output_data_dir":"/opt/ml/output/data","output_dir":"/opt/ml/output","output_intermediate_dir":"/opt/ml/output/intermediate","resource_config":{"current_host":"algo-1-a8iza","hosts":["algo-1-a8iza"]},"user_entry_point":"training_worker.py"}
SM_USER_ARGS=["--RLCOACH_PRESET","deepracer","--aws_region","us-east-1","--batch_size","64","--beta_entropy","0.01","--discount_factor","0.995","--e_greedy_value","0.05","--epsilon_steps","10000","--exploration_type","categorical","--loss_type","huber","--lr","0.0003","--model_metadata_s3_key","s3://bucket/custom_files/model_metadata.json","--num_episodes_between_training","20","--num_epochs","10","--s3_bucket","bucket","--s3_prefix","rl-deepracer-sagemaker","--sac_alpha","0.2","--stack_size","1","--term_cond_avg_score","400.0","--term_cond_max_episodes","3600"]
SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate
SM_HP_S3_BUCKET=bucket
SM_HP_S3_PREFIX=rl-deepracer-sagemaker
SM_HP_AWS_REGION=us-east-1
SM_HP_MODEL_METADATA_S3_KEY=s3://bucket/custom_files/model_metadata.json
SM_HP_RLCOACH_PRESET=deepracer
SM_HP_BATCH_SIZE=64
SM_HP_BETA_ENTROPY=0.01
SM_HP_DISCOUNT_FACTOR=0.995
SM_HP_E_GREEDY_VALUE=0.05
SM_HP_EPSILON_STEPS=10000
SM_HP_EXPLORATION_TYPE=categorical
SM_HP_LOSS_TYPE=huber
SM_HP_LR=0.0003
SM_HP_NUM_EPISODES_BETWEEN_TRAINING=20
SM_HP_NUM_EPOCHS=10
SM_HP_STACK_SIZE=1
SM_HP_TERM_COND_AVG_SCORE=400.0
SM_HP_TERM_COND_MAX_EPISODES=3600
SM_HP_SAC_ALPHA=0.2
PYTHONPATH=/usr/local/bin:/opt/amazon:/opt/ml/code:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages

Invoking script with the following command:

/usr/bin/python3 training_worker.py --RLCOACH_PRESET deepracer --aws_region us-east-1 --batch_size 64 --beta_entropy 0.01 --discount_factor 0.995 --e_greedy_value 0.05 --epsilon_steps 10000 --exploration_type categorical --loss_type huber --lr 0.0003 --model_metadata_s3_key s3://bucket/custom_files/model_metadata.json --num_episodes_between_training 20 --num_epochs 10 --s3_bucket bucket --s3_prefix rl-deepracer-sagemaker --sac_alpha 0.2 --stack_size 1 --term_cond_avg_score 400.0 --term_cond_max_episodes 3600


/usr/local/lib/python3.6/dist-packages/redis/utils.py:13: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography. The next release of cryptography will remove support for Python 3.6.
  import cryptography  # noqa
WARNING:tensorflow:From training_worker.py:47: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

WARNING:tensorflow:From training_worker.py:47: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.

Training Worker Args: Namespace(aws_region='us-east-1', checkpoint_dir='./checkpoint_sagemaker', cuda_visible_devices=None, environment_s3_key=None, framework='tensorflow', model_metadata_s3_key='s3://bucket/custom_files/model_metadata.json', preset_s3_key=None, pretrained_checkpoint='best', pretrained_checkpoint_dir='./pretrained_checkpoint_sagemaker', pretrained_s3_bucket=None, pretrained_s3_prefix='sagemaker', s3_bucket='bucket', s3_endpoint_url='http://minio:9000', s3_prefix='rl-deepracer-sagemaker')
S3 bucket: bucket 
 S3 prefix: rl-deepracer-sagemaker 
 S3 endpoint URL: http://minio:9000
[s3] Successfully downloaded model metadata                  from s3 key custom_files/model_metadata.json to local ./custom_files/agent/model_metadata.json.
Sensor list ['FRONT_FACING_CAMERA'], network DEEP_CONVOLUTIONAL_NETWORK, simapp_version 4.0, training_algorithm clipped_ppo, action_space_type continuous lidar_config {'num_sectors': 8, 'num_values_per_sector': 8, 'clipping_dist': 2.0}
Action space from file: {'steering_angle': {'high': 20, 'low': -20}, 'speed': {'high': 3.5, 'low': 0.8}}
Using the following hyper-parameters
{
  "batch_size": 64,
  "beta_entropy": 0.01,
  "discount_factor": 0.995,
  "e_greedy_value": 0.05,
  "epsilon_steps": 10000,
  "exploration_type": "categorical",
  "loss_type": "huber",
  "lr": 0.0003,
  "num_episodes_between_training": 20,
  "num_epochs": 10,
  "stack_size": 1,
  "term_cond_avg_score": 400.0,
  "term_cond_max_episodes": 3600
}
[s3] Successfully uploaded hyperparameters to                  s3 bucket bucket with s3 key rl-deepracer-sagemaker/ip/hyperparameters.json.
Hostname: algo-1-a8iza
[s3] Successfully uploaded ip address to                  s3 bucket bucket with s3 key rl-deepracer-sagemaker/ip/ip.json.
[s3] Successfully uploaded ip done to                  s3 bucket bucket with s3 key rl-deepracer-sagemaker/ip/done.
## Creating graph - name: MultiAgentGraphManager
## Start physics before creating graph
## Create graph
## Creating agent - name: agent
[RL] Created agent loggers
[RL] Dynamic import of memory:  "DeepRacerMemoryParameters" {
    "load_memory_from_file_path": null,
    "max_size": [
        "<MemoryGranularity.Transitions: 0>",
        1000000
    ],
    "n_step": -1,
    "shared_memory": false,
    "train_to_eval_ratio": 1
}

[RL] Dynamically imported of memory <markov.memories.deepracer_memory.DeepRacerMemory object at 0x7f5a501591d0>
[RL] Setting devices
[RL] Setting filters
[RL] Setting filter devices: numpy
[RL] Setting Phase
[RL] After setting Phase
[RL] Setting signals
[RL] Agent init successful
[RL] ActorCriticAgent init
[RL] ActorCriticAgent  init successful
## Created agent: agent
## Stop physics after creating graph
## Creating session
Creating regular session
2023-05-28 05:49:33.454119: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
AssignAdd: CPU 
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/global_step/Initializer/zeros (Const) 
  main_level/agent/main/online/global_step (VariableV2) /device:GPU:0
  main_level/agent/main/online/global_step/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/global_step/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/value (Const) /device:GPU:0
  main_level/agent/main/online/Adam (AssignAdd) /device:GPU:0
  main_level/agent/main/online/AssignAdd (AssignAdd) /device:GPU:0
  main_level/agent/main/target/AssignAdd (AssignAdd) /device:GPU:0

2023-05-28 05:49:33.454850: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
VariableV2: CPU 
Assign: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/Variable (VariableV2) /device:GPU:0
  main_level/agent/main/online/Variable/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/Variable/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign (Assign) /device:GPU:0

2023-05-28 05:49:33.455387: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
ApplyAdam: CPU 
RandomUniform: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_1 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.455847: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Mul: CPU XLA_CPU 
Const: CPU XLA_CPU 
ApplyAdam: CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/bias/Initializer/zeros (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/bias (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/bias/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_2 (Assign) /device:GPU:0
  main_level/agent/main/online/beta1_power/initial_value (Const) /device:GPU:0
  main_level/agent/main/online/beta1_power (VariableV2) /device:GPU:0
  main_level/agent/main/online/beta1_power/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/beta1_power/read (Identity) /device:GPU:0
  main_level/agent/main/online/beta2_power/initial_value (Const) /device:GPU:0
  main_level/agent/main/online/beta2_power (VariableV2) /device:GPU:0
  main_level/agent/main/online/beta2_power/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/beta2_power/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/bias/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/bias/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/bias/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/bias/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/bias/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/bias/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/bias/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/bias/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_0/bias/ApplyAdam (ApplyAdam) /device:GPU:0
  main_level/agent/main/online/Adam/mul (Mul) /device:GPU:0
  main_level/agent/main/online/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/Adam/mul_1 (Mul) /device:GPU:0
  main_level/agent/main/online/Adam/Assign_1 (Assign) /device:GPU:0

2023-05-28 05:49:33.456224: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
ApplyAdam: CPU 
RandomUniform: CPU XLA_CPU 
Fill: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_3 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam_1/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam_1/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam_1/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.456625: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
ApplyAdam: CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/bias/Initializer/zeros (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/bias (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/bias/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_4 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/bias/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/bias/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/bias/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/bias/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/bias/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/bias/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/bias/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/bias/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_2/bias/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.456973: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
ApplyAdam: CPU 
RandomUniform: CPU XLA_CPU 
Fill: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_5 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam_1/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam_1/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam_1/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.457237: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
ApplyAdam: CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/bias/Initializer/zeros (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/bias (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/bias/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_6 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/bias/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/bias/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/bias/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/bias/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/bias/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/bias/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/bias/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/bias/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_4/bias/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.457574: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
ApplyAdam: CPU 
RandomUniform: CPU XLA_CPU 
Fill: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_7 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam_1/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam_1/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam_1/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.457880: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
ApplyAdam: CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/bias/Initializer/zeros (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/bias (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/bias/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_8 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/bias/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/bias/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/bias/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/bias/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/bias/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/bias/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/bias/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/bias/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Conv2d_6/bias/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.458207: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
ApplyAdam: CPU 
RandomUniform: CPU XLA_CPU 
Fill: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_9 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Adam/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Adam/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Adam/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Adam_1/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Adam_1/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Adam_1/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.458496: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
ApplyAdam: CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/bias/Initializer/zeros (Const) 
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/bias (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/bias/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_10 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/bias/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/bias/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/bias/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/bias/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/bias/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/bias/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/bias/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/bias/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_0/FRONT_FACING_CAMERA/Dense_8/bias/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.458940: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
ApplyAdam: CPU 
RandomUniform: CPU XLA_CPU 
Fill: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_11 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Adam/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Adam/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Adam/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Adam_1/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Adam_1/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Adam_1/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/kernel/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.459261: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
ApplyAdam: CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/bias/Initializer/zeros (Const) 
  main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/bias (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/bias/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_12 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/bias/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/bias/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/bias/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/bias/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/bias/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/bias/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/bias/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/bias/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_0/middleware_fc_embedder/Dense_0/bias/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.459683: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
VariableV2: CPU 
Assign: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_0/Variable (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_0/Variable/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_0/Variable/read (Identity) /device:GPU:0

2023-05-28 05:49:33.460594: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Const: CPU XLA_CPU 
Identity: CPU XLA_CPU 
ApplyAdam: CPU 
VariableV2: CPU 
Assign: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_0/gradients_from_head_0-0_rescalers (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_0/gradients_from_head_0-0_rescalers/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_0/gradients_from_head_0-0_rescalers/read (Identity) /device:GPU:0
  main_level/agent/main/online/network_0/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/Assign_13 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/gradients_from_head_0-0_rescalers/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/gradients_from_head_0-0_rescalers/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/gradients_from_head_0-0_rescalers/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/gradients_from_head_0-0_rescalers/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/gradients_from_head_0-0_rescalers/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/gradients_from_head_0-0_rescalers/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/gradients_from_head_0-0_rescalers/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/gradients_from_head_0-0_rescalers/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_0/gradients_from_head_0-0_rescalers/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.461328: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
ApplyAdam: CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_0/v_values_head_0/output/kernel/Initializer/Const (Const) 
  main_level/agent/main/online/network_0/v_values_head_0/output/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_0/v_values_head_0/output/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_0/v_values_head_0/output/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_14 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/v_values_head_0/output/kernel/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/v_values_head_0/output/kernel/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/v_values_head_0/output/kernel/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/v_values_head_0/output/kernel/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/v_values_head_0/output/kernel/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/v_values_head_0/output/kernel/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/v_values_head_0/output/kernel/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/v_values_head_0/output/kernel/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_0/v_values_head_0/output/kernel/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.463638: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
ApplyAdam: CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_0/v_values_head_0/output/bias/Initializer/zeros (Const) 
  main_level/agent/main/online/network_0/v_values_head_0/output/bias (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_0/v_values_head_0/output/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_0/v_values_head_0/output/bias/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_15 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/v_values_head_0/output/bias/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/v_values_head_0/output/bias/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/v_values_head_0/output/bias/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/v_values_head_0/output/bias/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/v_values_head_0/output/bias/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/v_values_head_0/output/bias/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/v_values_head_0/output/bias/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_0/v_values_head_0/output/bias/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_0/v_values_head_0/output/bias/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.464641: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
ApplyAdam: CPU 
RandomUniform: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_16 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.465020: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
ApplyAdam: CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/bias/Initializer/zeros (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/bias (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/bias/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_17 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/bias/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/bias/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/bias/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/bias/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/bias/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/bias/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/bias/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/bias/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_0/bias/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.465388: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
ApplyAdam: CPU 
RandomUniform: CPU XLA_CPU 
Fill: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_18 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam_1/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam_1/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam_1/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.466206: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
ApplyAdam: CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/bias/Initializer/zeros (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/bias (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/bias/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_19 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/bias/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/bias/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/bias/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/bias/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/bias/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/bias/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/bias/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/bias/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_2/bias/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.466852: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
ApplyAdam: CPU 
RandomUniform: CPU XLA_CPU 
Fill: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_20 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam_1/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam_1/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam_1/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.467649: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
ApplyAdam: CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/bias/Initializer/zeros (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/bias (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/bias/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_21 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/bias/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/bias/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/bias/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/bias/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/bias/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/bias/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/bias/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/bias/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_4/bias/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.494590: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
ApplyAdam: CPU 
RandomUniform: CPU XLA_CPU 
Fill: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_22 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam_1/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam_1/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam_1/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.495407: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
ApplyAdam: CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/bias/Initializer/zeros (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/bias (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/bias/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_23 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/bias/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/bias/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/bias/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/bias/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/bias/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/bias/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/bias/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/bias/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Conv2d_6/bias/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.496658: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
ApplyAdam: CPU 
RandomUniform: CPU XLA_CPU 
Fill: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_24 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Adam/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Adam/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Adam/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Adam_1/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Adam_1/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Adam_1/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.497081: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
ApplyAdam: CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/bias/Initializer/zeros (Const) 
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/bias (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/bias/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_25 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/bias/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/bias/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/bias/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/bias/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/bias/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/bias/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/bias/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/bias/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_1/FRONT_FACING_CAMERA/Dense_8/bias/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.498106: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
ApplyAdam: CPU 
RandomUniform: CPU XLA_CPU 
Fill: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_26 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Adam/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Adam/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Adam/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Adam_1/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Adam_1/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Adam_1/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/kernel/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.498551: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
ApplyAdam: CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/bias/Initializer/zeros (Const) 
  main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/bias (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/bias/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_27 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/bias/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/bias/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/bias/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/bias/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/bias/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/bias/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/bias/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/bias/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_1/middleware_fc_embedder/Dense_0/bias/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.498779: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
VariableV2: CPU 
Assign: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_1/Variable (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_1/Variable/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_1/Variable/read (Identity) /device:GPU:0

2023-05-28 05:49:33.499065: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Const: CPU XLA_CPU 
Identity: CPU XLA_CPU 
ApplyAdam: CPU 
VariableV2: CPU 
Assign: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_1/gradients_from_head_0-0_rescalers (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_1/gradients_from_head_0-0_rescalers/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_1/gradients_from_head_0-0_rescalers/read (Identity) /device:GPU:0
  main_level/agent/main/online/network_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/Assign_28 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/gradients_from_head_0-0_rescalers/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/gradients_from_head_0-0_rescalers/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/gradients_from_head_0-0_rescalers/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/gradients_from_head_0-0_rescalers/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/gradients_from_head_0-0_rescalers/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/gradients_from_head_0-0_rescalers/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/gradients_from_head_0-0_rescalers/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/gradients_from_head_0-0_rescalers/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_1/gradients_from_head_0-0_rescalers/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.499342: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Fill: CPU XLA_CPU 
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
ApplyAdam: CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_1/ppo_head_0/policy_mean/kernel/Initializer/Const (Const) 
  main_level/agent/main/online/network_1/ppo_head_0/policy_mean/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_1/ppo_head_0/policy_mean/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_1/ppo_head_0/policy_mean/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_29 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/kernel/Adam/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/kernel/Adam/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/kernel/Adam/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/kernel/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/kernel/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/kernel/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/kernel/Adam_1/Initializer/zeros/shape_as_tensor (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/kernel/Adam_1/Initializer/zeros/Const (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/kernel/Adam_1/Initializer/zeros (Fill) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/kernel/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/kernel/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/kernel/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_1/ppo_head_0/policy_mean/kernel/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.499607: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
ApplyAdam: CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_1/ppo_head_0/policy_mean/bias/Initializer/zeros (Const) 
  main_level/agent/main/online/network_1/ppo_head_0/policy_mean/bias (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_1/ppo_head_0/policy_mean/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_1/ppo_head_0/policy_mean/bias/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_30 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/bias/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/bias/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/bias/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/bias/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/bias/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/bias/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/bias/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_mean/bias/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_1/ppo_head_0/policy_mean/bias/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.499903: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Const: CPU XLA_CPU 
Identity: CPU XLA_CPU 
ApplyAdam: CPU 
VariableV2: CPU 
Assign: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/online/network_1/ppo_head_0/policy_log_std (VariableV2) /device:GPU:0
  main_level/agent/main/online/network_1/ppo_head_0/policy_log_std/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/network_1/ppo_head_0/policy_log_std/read (Identity) /device:GPU:0
  main_level/agent/main/online/Assign_31 (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_log_std/Adam/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_log_std/Adam (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_log_std/Adam/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_log_std/Adam/read (Identity) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_log_std/Adam_1/Initializer/zeros (Const) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_log_std/Adam_1 (VariableV2) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_log_std/Adam_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/online/main_level/agent/main/online/network_1/ppo_head_0/policy_log_std/Adam_1/read (Identity) /device:GPU:0
  main_level/agent/main/online/Adam/update_main_level/agent/main/online/network_1/ppo_head_0/policy_log_std/ApplyAdam (ApplyAdam) /device:GPU:0

2023-05-28 05:49:33.505479: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
VariableV2: CPU 
Assign: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/Variable (VariableV2) /device:GPU:0
  main_level/agent/main/target/Variable/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/Variable/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign (Assign) /device:GPU:0

2023-05-28 05:49:33.506803: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
RandomUniform: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_0/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_1 (Assign) /device:GPU:0

2023-05-28 05:49:33.514190: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_0/bias/Initializer/zeros (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_0/bias (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_0/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_0/bias/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_2 (Assign) /device:GPU:0

2023-05-28 05:49:33.515209: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
RandomUniform: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_2/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_3 (Assign) /device:GPU:0

2023-05-28 05:49:33.515778: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_2/bias/Initializer/zeros (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_2/bias (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_2/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_2/bias/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_4 (Assign) /device:GPU:0

2023-05-28 05:49:33.516515: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
RandomUniform: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_4/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_5 (Assign) /device:GPU:0

2023-05-28 05:49:33.517037: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_4/bias/Initializer/zeros (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_4/bias (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_4/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_4/bias/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_6 (Assign) /device:GPU:0

2023-05-28 05:49:33.517894: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
RandomUniform: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_6/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_7 (Assign) /device:GPU:0

2023-05-28 05:49:33.518869: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_6/bias/Initializer/zeros (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_6/bias (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_6/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Conv2d_6/bias/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_8 (Assign) /device:GPU:0

2023-05-28 05:49:33.519585: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
RandomUniform: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Dense_8/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Dense_8/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_9 (Assign) /device:GPU:0

2023-05-28 05:49:33.520223: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Dense_8/bias/Initializer/zeros (Const) 
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Dense_8/bias (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Dense_8/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_0/FRONT_FACING_CAMERA/Dense_8/bias/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_10 (Assign) /device:GPU:0

2023-05-28 05:49:33.523963: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
RandomUniform: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_0/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/target/network_0/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/target/network_0/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/target/network_0/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/target/network_0/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/target/network_0/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/target/network_0/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/target/network_0/middleware_fc_embedder/Dense_0/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_0/middleware_fc_embedder/Dense_0/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_0/middleware_fc_embedder/Dense_0/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_11 (Assign) /device:GPU:0

2023-05-28 05:49:33.528220: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_0/middleware_fc_embedder/Dense_0/bias/Initializer/zeros (Const) 
  main_level/agent/main/target/network_0/middleware_fc_embedder/Dense_0/bias (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_0/middleware_fc_embedder/Dense_0/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_0/middleware_fc_embedder/Dense_0/bias/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_12 (Assign) /device:GPU:0

2023-05-28 05:49:33.529438: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
VariableV2: CPU 
Assign: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_0/Variable (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_0/Variable/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_0/Variable/read (Identity) /device:GPU:0

2023-05-28 05:49:33.530851: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
VariableV2: CPU 
Assign: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_0/gradients_from_head_0-0_rescalers (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_0/gradients_from_head_0-0_rescalers/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_0/gradients_from_head_0-0_rescalers/read (Identity) /device:GPU:0
  main_level/agent/main/target/network_0/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/Assign_13 (Assign) /device:GPU:0

2023-05-28 05:49:33.532449: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_0/v_values_head_0/output/kernel/Initializer/Const (Const) 
  main_level/agent/main/target/network_0/v_values_head_0/output/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_0/v_values_head_0/output/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_0/v_values_head_0/output/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_14 (Assign) /device:GPU:0

2023-05-28 05:49:33.533285: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_0/v_values_head_0/output/bias/Initializer/zeros (Const) 
  main_level/agent/main/target/network_0/v_values_head_0/output/bias (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_0/v_values_head_0/output/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_0/v_values_head_0/output/bias/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_15 (Assign) /device:GPU:0

2023-05-28 05:49:33.536026: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
RandomUniform: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_0/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_16 (Assign) /device:GPU:0

2023-05-28 05:49:33.546449: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_0/bias/Initializer/zeros (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_0/bias (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_0/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_0/bias/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_17 (Assign) /device:GPU:0

2023-05-28 05:49:33.548899: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
RandomUniform: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_2/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_18 (Assign) /device:GPU:0

2023-05-28 05:49:33.560610: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_2/bias/Initializer/zeros (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_2/bias (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_2/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_2/bias/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_19 (Assign) /device:GPU:0

2023-05-28 05:49:33.562019: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
RandomUniform: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_4/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_20 (Assign) /device:GPU:0

2023-05-28 05:49:33.564882: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_4/bias/Initializer/zeros (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_4/bias (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_4/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_4/bias/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_21 (Assign) /device:GPU:0

2023-05-28 05:49:33.566160: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
RandomUniform: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_6/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_22 (Assign) /device:GPU:0

2023-05-28 05:49:33.566634: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_6/bias/Initializer/zeros (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_6/bias (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_6/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Conv2d_6/bias/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_23 (Assign) /device:GPU:0

2023-05-28 05:49:33.567017: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
RandomUniform: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Dense_8/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Dense_8/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_24 (Assign) /device:GPU:0

2023-05-28 05:49:33.567339: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Dense_8/bias/Initializer/zeros (Const) 
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Dense_8/bias (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Dense_8/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_1/FRONT_FACING_CAMERA/Dense_8/bias/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_25 (Assign) /device:GPU:0

2023-05-28 05:49:33.567661: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Assign: CPU 
RandomUniform: CPU XLA_CPU 
Const: CPU XLA_CPU 
Mul: CPU XLA_CPU 
Sub: CPU XLA_CPU 
Add: CPU XLA_CPU 
Identity: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_1/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/shape (Const) 
  main_level/agent/main/target/network_1/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/min (Const) 
  main_level/agent/main/target/network_1/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/max (Const) 
  main_level/agent/main/target/network_1/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/RandomUniform (RandomUniform) 
  main_level/agent/main/target/network_1/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/sub (Sub) 
  main_level/agent/main/target/network_1/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform/mul (Mul) 
  main_level/agent/main/target/network_1/middleware_fc_embedder/Dense_0/kernel/Initializer/random_uniform (Add) 
  main_level/agent/main/target/network_1/middleware_fc_embedder/Dense_0/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_1/middleware_fc_embedder/Dense_0/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_1/middleware_fc_embedder/Dense_0/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_26 (Assign) /device:GPU:0

2023-05-28 05:49:33.567946: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_1/middleware_fc_embedder/Dense_0/bias/Initializer/zeros (Const) 
  main_level/agent/main/target/network_1/middleware_fc_embedder/Dense_0/bias (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_1/middleware_fc_embedder/Dense_0/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_1/middleware_fc_embedder/Dense_0/bias/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_27 (Assign) /device:GPU:0

2023-05-28 05:49:33.568155: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
VariableV2: CPU 
Assign: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_1/Variable (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_1/Variable/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_1/Variable/read (Identity) /device:GPU:0

2023-05-28 05:49:33.568570: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
VariableV2: CPU 
Assign: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_1/gradients_from_head_0-0_rescalers (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_1/gradients_from_head_0-0_rescalers/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_1/gradients_from_head_0-0_rescalers/read (Identity) /device:GPU:0
  main_level/agent/main/target/network_1/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/Assign_28 (Assign) /device:GPU:0

2023-05-28 05:49:33.568864: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_1/ppo_head_0/policy_mean/kernel/Initializer/Const (Const) 
  main_level/agent/main/target/network_1/ppo_head_0/policy_mean/kernel (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_1/ppo_head_0/policy_mean/kernel/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_1/ppo_head_0/policy_mean/kernel/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_29 (Assign) /device:GPU:0

2023-05-28 05:49:33.569272: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
Assign: CPU 
Const: CPU XLA_CPU 
VariableV2: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_1/ppo_head_0/policy_mean/bias/Initializer/zeros (Const) 
  main_level/agent/main/target/network_1/ppo_head_0/policy_mean/bias (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_1/ppo_head_0/policy_mean/bias/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_1/ppo_head_0/policy_mean/bias/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_30 (Assign) /device:GPU:0

2023-05-28 05:49:33.569573: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
See below for details of this colocation group:
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: CPU XLA_CPU 
VariableV2: CPU 
Assign: CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  main_level/agent/main/target/network_1/ppo_head_0/policy_log_std (VariableV2) /device:GPU:0
  main_level/agent/main/target/network_1/ppo_head_0/policy_log_std/Assign (Assign) /device:GPU:0
  main_level/agent/main/target/network_1/ppo_head_0/policy_log_std/read (Identity) /device:GPU:0
  main_level/agent/main/target/Assign_31 (Assign) /device:GPU:0

Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/0_Step-0.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 0
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
Unable to find deepracer checkpoint json
Unable to find the best deepracer checkpoint number. Getting the last checkpoint number
Unable to find deepracer checkpoint json
Unable to find the last deepracer checkpoint number.
Unable to find deepracer checkpoint json
Unable to find the last deepracer checkpoint number.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_0.pb
Best checkpoint number: -1, Last checkpoint number: -1
Copying the frozen checkpoint from ./frozen_models/agent/model_0.pb to /opt/ml/model/agent/model.pb.
Unable to find deepracer checkpoint json
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
[s3] Successfully uploaded .ready to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.ready.
DoorMan: installing SIGINT, SIGTERM
Training> Name=main_level/agent, Worker=0, Episode=1, Total reward=0.15, Steps=16, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=2, Total reward=0.16, Steps=33, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=3, Total reward=0.11, Steps=45, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=4, Total reward=6.45, Steps=62, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=5, Total reward=11.41, Steps=82, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=6, Total reward=28.15, Steps=120, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=7, Total reward=8.52, Steps=164, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=8, Total reward=0.27, Steps=192, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=9, Total reward=0.17, Steps=210, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=10, Total reward=0.91, Steps=231, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=11, Total reward=26.53, Steps=280, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=12, Total reward=13.76, Steps=308, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=13, Total reward=5.96, Steps=333, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=14, Total reward=0.16, Steps=350, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=15, Total reward=0.36, Steps=387, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=16, Total reward=0.18, Steps=406, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=17, Total reward=0.25, Steps=432, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=18, Total reward=0.18, Steps=451, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=19, Total reward=0.62, Steps=514, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=20, Total reward=0.2, Steps=535, Training iteration=0
Policy training> Surrogate loss=-0.00659149931743741, KL divergence=6.705394753225846e-06, Entropy=2.8378829956054688, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=0.0030210218392312527, KL divergence=0.00046602991642430425, Entropy=2.837815761566162, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.00806360598653555, KL divergence=0.014566123485565186, Entropy=2.8375988006591797, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.013446709141135216, KL divergence=0.001996837556362152, Entropy=2.837254524230957, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.0186123289167881, KL divergence=0.009717581793665886, Entropy=2.8372690677642822, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.016562480479478836, KL divergence=0.00864577479660511, Entropy=2.836909770965576, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=0.001562054269015789, KL divergence=0.005926021374762058, Entropy=2.8359601497650146, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.008855123072862625, KL divergence=0.007331963628530502, Entropy=2.8350460529327393, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=0.005276177078485489, KL divergence=0.0063648223876953125, Entropy=2.8340671062469482, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.027923662215471268, KL divergence=0.0065933833830058575, Entropy=2.8331308364868164, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/1_Step-535.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 1
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_1.pb
Best checkpoint number: 0, Last checkpoint number: 0
Copying the frozen checkpoint from ./frozen_models/agent/model_0.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Training> Name=main_level/agent, Worker=0, Episode=21, Total reward=0.35, Steps=571, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=22, Total reward=0.16, Steps=588, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=23, Total reward=0.14, Steps=603, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=24, Total reward=14.34, Steps=626, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=25, Total reward=10.94, Steps=646, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=26, Total reward=14.9, Steps=668, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=27, Total reward=13.18, Steps=689, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=28, Total reward=0.15, Steps=705, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=29, Total reward=0.13, Steps=719, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=30, Total reward=0.6, Steps=741, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=31, Total reward=18.71, Steps=788, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=32, Total reward=20.8, Steps=828, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=33, Total reward=8.01, Steps=859, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=34, Total reward=0.12, Steps=872, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=35, Total reward=0.37, Steps=910, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=36, Total reward=0.3, Steps=941, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=37, Total reward=0.35, Steps=977, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=38, Total reward=0.27, Steps=1005, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=39, Total reward=0.29, Steps=1035, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=40, Total reward=0.25, Steps=1061, Training iteration=1
Policy training> Surrogate loss=0.020137844607234, KL divergence=0.0002809286233969033, Entropy=2.8322396278381348, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.004882359877228737, KL divergence=0.0008125266758725047, Entropy=2.832012414932251, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.013575316406786442, KL divergence=0.0015699320938438177, Entropy=2.8321146965026855, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=0.005527377128601074, KL divergence=0.0030050682835280895, Entropy=2.832383394241333, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=0.012590546160936356, KL divergence=0.002112324582412839, Entropy=2.8325037956237793, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=0.014640259556472301, KL divergence=0.0021456433460116386, Entropy=2.8325343132019043, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.0030365977436304092, KL divergence=0.002319040708243847, Entropy=2.832655429840088, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.011009017005562782, KL divergence=0.0028061456978321075, Entropy=2.832775592803955, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.013968899846076965, KL divergence=0.0037068782839924097, Entropy=2.8330748081207275, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.007609976455569267, KL divergence=0.004836964420974255, Entropy=2.8327932357788086, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/2_Step-1061.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 2
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_2.pb
Best checkpoint number: 0, Last checkpoint number: 0
Copying the frozen checkpoint from ./frozen_models/agent/model_0.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Training> Name=main_level/agent, Worker=0, Episode=41, Total reward=0.35, Steps=1097, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=42, Total reward=0.14, Steps=1112, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=43, Total reward=0.12, Steps=1125, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=44, Total reward=5.29, Steps=1138, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=45, Total reward=8.36, Steps=1152, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=46, Total reward=22.5, Steps=1188, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=47, Total reward=14.87, Steps=1209, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=48, Total reward=0.31, Steps=1241, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=49, Total reward=8.81, Steps=1277, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=50, Total reward=0.15, Steps=1293, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=51, Total reward=10.48, Steps=1312, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=52, Total reward=18.03, Steps=1353, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=53, Total reward=5.26, Steps=1383, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=54, Total reward=0.16, Steps=1400, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=55, Total reward=0.32, Steps=1433, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=56, Total reward=0.16, Steps=1450, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=57, Total reward=0.33, Steps=1484, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=58, Total reward=0.37, Steps=1522, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=59, Total reward=0.25, Steps=1548, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=60, Total reward=0.17, Steps=1566, Training iteration=2
Policy training> Surrogate loss=-0.012714019976556301, KL divergence=0.005740799009799957, Entropy=2.8324801921844482, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=0.00968344509601593, KL divergence=0.005162188317626715, Entropy=2.8322479724884033, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=0.021860385313630104, KL divergence=0.006814278196543455, Entropy=2.832113265991211, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.009109852835536003, KL divergence=0.027137963101267815, Entropy=2.832042932510376, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=0.015866165980696678, KL divergence=0.009863096289336681, Entropy=2.8320415019989014, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.013046056032180786, KL divergence=0.006963466759771109, Entropy=2.8317837715148926, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.004960642196238041, KL divergence=0.017395323142409325, Entropy=2.8315393924713135, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.0009245489491149783, KL divergence=0.008776194415986538, Entropy=2.831609010696411, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=0.030269837006926537, KL divergence=0.006265333853662014, Entropy=2.8316142559051514, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.0225355327129364, KL divergence=0.008730629459023476, Entropy=2.8313498497009277, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/3_Step-1566.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 3
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_3.pb
Best checkpoint number: 1, Last checkpoint number: 1
Copying the frozen checkpoint from ./frozen_models/agent/model_1.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Training> Name=main_level/agent, Worker=0, Episode=61, Total reward=0.14, Steps=1581, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=62, Total reward=0.25, Steps=1607, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=63, Total reward=0.89, Steps=1623, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=64, Total reward=4.67, Steps=1636, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=65, Total reward=20.87, Steps=1674, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=66, Total reward=13.61, Steps=1699, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=67, Total reward=12.99, Steps=1717, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=68, Total reward=0.22, Steps=1740, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=69, Total reward=0.18, Steps=1759, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=70, Total reward=0.43, Steps=1777, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=71, Total reward=13.52, Steps=1815, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=72, Total reward=12.74, Steps=1842, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=73, Total reward=3.87, Steps=1859, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=74, Total reward=0.32, Steps=1890, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=75, Total reward=0.53, Steps=1944, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=76, Total reward=0.35, Steps=1980, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=77, Total reward=0.36, Steps=2017, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=78, Total reward=0.24, Steps=2042, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=79, Total reward=0.28, Steps=2071, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=80, Total reward=0.18, Steps=2090, Training iteration=3
Policy training> Surrogate loss=-0.0005992671940475702, KL divergence=0.00955438707023859, Entropy=2.83148455619812, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.02009112946689129, KL divergence=0.016117846593260765, Entropy=2.832050085067749, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.021006636321544647, KL divergence=0.007459909655153751, Entropy=2.832573890686035, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.03297949582338333, KL divergence=0.014696149155497551, Entropy=2.8327107429504395, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.03311646357178688, KL divergence=0.01901424117386341, Entropy=2.8327383995056152, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.04565003886818886, KL divergence=0.02532290853559971, Entropy=2.8320891857147217, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.03987037390470505, KL divergence=0.029582953080534935, Entropy=2.8313331604003906, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.060588184744119644, KL divergence=0.024266881868243217, Entropy=2.8304715156555176, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.05660809576511383, KL divergence=0.03930564597249031, Entropy=2.829223155975342, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.064459428191185, KL divergence=0.046710044145584106, Entropy=2.8274900913238525, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/4_Step-2090.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 4
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_4.pb
Best checkpoint number: 2, Last checkpoint number: 2
Copying the frozen checkpoint from ./frozen_models/agent/model_2.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'0'}
Training> Name=main_level/agent, Worker=0, Episode=81, Total reward=0.23, Steps=2114, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=82, Total reward=0.33, Steps=2148, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=83, Total reward=0.21, Steps=2166, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=84, Total reward=10.45, Steps=2186, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=85, Total reward=24.47, Steps=2231, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=86, Total reward=29.34, Steps=2301, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=87, Total reward=16.62, Steps=2324, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=88, Total reward=0.15, Steps=2340, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=89, Total reward=7.68, Steps=2365, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=90, Total reward=2.8, Steps=2401, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=91, Total reward=14.41, Steps=2443, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=92, Total reward=11.8, Steps=2469, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=93, Total reward=4.04, Steps=2487, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=94, Total reward=0.18, Steps=2506, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=95, Total reward=0.24, Steps=2531, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=96, Total reward=0.15, Steps=2547, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=97, Total reward=0.51, Steps=2599, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=98, Total reward=0.23, Steps=2623, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=99, Total reward=0.23, Steps=2647, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=100, Total reward=0.11, Steps=2659, Training iteration=4
Policy training> Surrogate loss=0.024900568649172783, KL divergence=0.017209060490131378, Entropy=2.826239585876465, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.025976715609431267, KL divergence=0.030204346403479576, Entropy=2.8253955841064453, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.01704913191497326, KL divergence=0.022597653791308403, Entropy=2.8240861892700195, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.04273350536823273, KL divergence=0.0322607085108757, Entropy=2.823150157928467, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.06183268874883652, KL divergence=0.035685762763023376, Entropy=2.8221023082733154, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.04824239760637283, KL divergence=0.040021058171987534, Entropy=2.8209753036499023, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.06786022335290909, KL divergence=0.044925760477781296, Entropy=2.8199408054351807, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.05497201159596443, KL divergence=0.059897344559431076, Entropy=2.818972587585449, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.06063006818294525, KL divergence=0.057140521705150604, Entropy=2.8179454803466797, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.056472692638635635, KL divergence=0.06138170510530472, Entropy=2.8166966438293457, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/5_Step-2659.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 5
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_5.pb
Best checkpoint number: 3, Last checkpoint number: 3
Copying the frozen checkpoint from ./frozen_models/agent/model_3.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'1'}
Training> Name=main_level/agent, Worker=0, Episode=101, Total reward=2.7, Steps=2695, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=102, Total reward=0.91, Steps=2733, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=103, Total reward=1.42, Steps=2753, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=104, Total reward=22.28, Steps=2785, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=105, Total reward=47.57, Steps=2849, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=106, Total reward=28.12, Steps=2891, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=107, Total reward=9.92, Steps=2907, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=108, Total reward=0.34, Steps=2942, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=109, Total reward=11.41, Steps=2980, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=110, Total reward=9.7, Steps=3007, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=111, Total reward=15.92, Steps=3044, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=112, Total reward=14.55, Steps=3074, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=113, Total reward=1.64, Steps=3088, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=114, Total reward=0.28, Steps=3117, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=115, Total reward=0.24, Steps=3142, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=116, Total reward=0.19, Steps=3162, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=117, Total reward=0.29, Steps=3192, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=118, Total reward=0.18, Steps=3211, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=119, Total reward=0.22, Steps=3234, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=120, Total reward=0.15, Steps=3250, Training iteration=5
Policy training> Surrogate loss=0.01235470175743103, KL divergence=0.024410178884863853, Entropy=2.8154425621032715, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.007966158911585808, KL divergence=0.0310799703001976, Entropy=2.815340280532837, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.0422513373196125, KL divergence=0.05471257492899895, Entropy=2.8153939247131348, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.05920423939824104, KL divergence=0.03990776464343071, Entropy=2.8146257400512695, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.06722542643547058, KL divergence=0.04278423264622688, Entropy=2.8136167526245117, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.0843740776181221, KL divergence=0.060315173119306564, Entropy=2.8125884532928467, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.11034785211086273, KL divergence=0.07187318801879883, Entropy=2.8116631507873535, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.1108543798327446, KL divergence=0.09204188734292984, Entropy=2.8106727600097656, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.10048185288906097, KL divergence=0.10559740662574768, Entropy=2.8093841075897217, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.09755513817071915, KL divergence=0.11025868356227875, Entropy=2.807664155960083, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/6_Step-3250.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 6
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_6.pb
Best checkpoint number: 4, Last checkpoint number: 4
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'2'}
Training> Name=main_level/agent, Worker=0, Episode=121, Total reward=0.62, Steps=3313, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=122, Total reward=9.02, Steps=3356, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=123, Total reward=2.26, Steps=3375, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=124, Total reward=33.81, Steps=3423, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=125, Total reward=34.16, Steps=3477, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=126, Total reward=32.27, Steps=3522, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=127, Total reward=13.05, Steps=3555, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=128, Total reward=20.33, Steps=3653, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=129, Total reward=8.35, Steps=3681, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=130, Total reward=8.28, Steps=3701, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=131, Total reward=28.6, Steps=3763, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=132, Total reward=24.64, Steps=3812, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=133, Total reward=5.73, Steps=3830, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=134, Total reward=0.21, Steps=3852, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=135, Total reward=0.3, Steps=3883, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=136, Total reward=0.21, Steps=3905, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=137, Total reward=0.17, Steps=3923, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=138, Total reward=0.2, Steps=3944, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=139, Total reward=0.37, Steps=3982, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=140, Total reward=14.76, Steps=4085, Training iteration=6
Policy training> Surrogate loss=0.014592001214623451, KL divergence=0.02553006261587143, Entropy=2.80507755279541, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.03003469854593277, KL divergence=0.05054296553134918, Entropy=2.8031249046325684, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.04955367371439934, KL divergence=0.06289800256490707, Entropy=2.8017616271972656, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.0650956854224205, KL divergence=0.06827779114246368, Entropy=2.7992680072784424, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.07997874170541763, KL divergence=0.08707677572965622, Entropy=2.7954812049865723, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.0956823080778122, KL divergence=0.09135710448026657, Entropy=2.7915196418762207, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.10697270929813385, KL divergence=0.11214765906333923, Entropy=2.786163568496704, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.11091714352369308, KL divergence=0.12080374360084534, Entropy=2.780440092086792, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.11424820125102997, KL divergence=0.1333322376012802, Entropy=2.775571584701538, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.1265733391046524, KL divergence=0.13413606584072113, Entropy=2.771021604537964, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/7_Step-4085.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 7
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_7.pb
Best checkpoint number: 4, Last checkpoint number: 5
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'3'}
Training> Name=main_level/agent, Worker=0, Episode=141, Total reward=0.25, Steps=4111, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=142, Total reward=0.44, Steps=4156, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=143, Total reward=0.15, Steps=4172, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=144, Total reward=29.66, Steps=4215, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=145, Total reward=11.73, Steps=4237, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=146, Total reward=29.49, Steps=4279, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=147, Total reward=12.81, Steps=4316, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=148, Total reward=0.53, Steps=4362, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=149, Total reward=7.87, Steps=4391, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=150, Total reward=26.6, Steps=4459, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=151, Total reward=6.05, Steps=4484, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=152, Total reward=18.19, Steps=4530, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=153, Total reward=6.37, Steps=4572, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=154, Total reward=0.3, Steps=4603, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=155, Total reward=0.29, Steps=4633, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=156, Total reward=0.38, Steps=4672, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=157, Total reward=0.39, Steps=4712, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=158, Total reward=0.79, Steps=4792, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=159, Total reward=0.28, Steps=4821, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=160, Total reward=0.43, Steps=4865, Training iteration=7
Policy training> Surrogate loss=0.03232400491833687, KL divergence=0.05061032250523567, Entropy=2.7678792476654053, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.0199048463255167, KL divergence=0.09919863194227219, Entropy=2.7685136795043945, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.0506339929997921, KL divergence=0.08359060436487198, Entropy=2.770458221435547, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.06293675303459167, KL divergence=0.08568666130304337, Entropy=2.7721681594848633, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.07904691249132156, KL divergence=0.08006654679775238, Entropy=2.77172589302063, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.09188073128461838, KL divergence=0.09421689063310623, Entropy=2.771590232849121, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.09591740369796753, KL divergence=0.1015927717089653, Entropy=2.770915985107422, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.09148142486810684, KL divergence=0.1094890609383583, Entropy=2.7694718837738037, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.10253387689590454, KL divergence=0.11726013571023941, Entropy=2.767890214920044, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.09582307189702988, KL divergence=0.11165212839841843, Entropy=2.765911817550659, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/8_Step-4865.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 8
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_8.pb
Best checkpoint number: 4, Last checkpoint number: 6
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'5'}
Training> Name=main_level/agent, Worker=0, Episode=161, Total reward=0.43, Steps=4909, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=162, Total reward=0.29, Steps=4939, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=163, Total reward=0.8, Steps=4958, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=164, Total reward=19.45, Steps=4986, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=165, Total reward=39.83, Steps=5044, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=166, Total reward=15.34, Steps=5073, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=167, Total reward=12.43, Steps=5093, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=168, Total reward=0.19, Steps=5113, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=169, Total reward=6.34, Steps=5136, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=170, Total reward=8.71, Steps=5182, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=171, Total reward=4.18, Steps=5212, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=172, Total reward=9.89, Steps=5236, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=173, Total reward=4.98, Steps=5258, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=174, Total reward=0.17, Steps=5276, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=175, Total reward=0.27, Steps=5304, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=176, Total reward=0.58, Steps=5363, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=177, Total reward=0.35, Steps=5399, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=178, Total reward=0.2, Steps=5420, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=179, Total reward=0.18, Steps=5439, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=180, Total reward=0.24, Steps=5464, Training iteration=8
Policy training> Surrogate loss=0.018795117735862732, KL divergence=0.02353021129965782, Entropy=2.7640721797943115, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.07362110912799835, KL divergence=0.06461482495069504, Entropy=2.763033866882324, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.09133575856685638, KL divergence=0.1349012404680252, Entropy=2.7623982429504395, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.09087220579385757, KL divergence=0.14353197813034058, Entropy=2.7620222568511963, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.10785789042711258, KL divergence=0.14574643969535828, Entropy=2.761017322540283, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.10461040586233139, KL divergence=0.19880907237529755, Entropy=2.759557008743286, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.1051083579659462, KL divergence=0.22423478960990906, Entropy=2.758345365524292, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.09438307583332062, KL divergence=0.20014482736587524, Entropy=2.7574663162231445, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.10704697668552399, KL divergence=0.2017216831445694, Entropy=2.7564611434936523, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.10670207440853119, KL divergence=0.2506531774997711, Entropy=2.7556002140045166, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/9_Step-5464.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 9
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_9.pb
Best checkpoint number: 4, Last checkpoint number: 7
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'6'}
Training> Name=main_level/agent, Worker=0, Episode=181, Total reward=0.25, Steps=5490, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=182, Total reward=0.86, Steps=5519, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=183, Total reward=0.19, Steps=5535, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=184, Total reward=50.88, Steps=5606, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=185, Total reward=15.95, Steps=5630, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=186, Total reward=28.5, Steps=5668, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=187, Total reward=10.24, Steps=5721, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=188, Total reward=0.22, Steps=5744, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=189, Total reward=5.38, Steps=5760, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=190, Total reward=38.13, Steps=5868, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=191, Total reward=17.81, Steps=5907, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=192, Total reward=12.2, Steps=5939, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=193, Total reward=5.31, Steps=5993, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=194, Total reward=0.14, Steps=6008, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=195, Total reward=0.18, Steps=6027, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=196, Total reward=0.58, Steps=6086, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=197, Total reward=0.29, Steps=6116, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=198, Total reward=0.27, Steps=6144, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=199, Total reward=0.17, Steps=6162, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=200, Total reward=0.46, Steps=6209, Training iteration=9
Policy training> Surrogate loss=0.018090905621647835, KL divergence=0.024399099871516228, Entropy=2.754697322845459, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.024946296587586403, KL divergence=0.11029756814241409, Entropy=2.7545595169067383, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.08997628837823868, KL divergence=0.14530043303966522, Entropy=2.755084753036499, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.09580123424530029, KL divergence=0.16282208263874054, Entropy=2.7541558742523193, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.11507409065961838, KL divergence=0.20659106969833374, Entropy=2.7525904178619385, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.12531723082065582, KL divergence=0.2156718671321869, Entropy=2.75032377243042, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.14448930323123932, KL divergence=0.21990971267223358, Entropy=2.748225212097168, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.13167880475521088, KL divergence=0.24913085997104645, Entropy=2.74607253074646, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.12482580542564392, KL divergence=0.2673226594924927, Entropy=2.7434403896331787, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.13225285708904266, KL divergence=0.3007722795009613, Entropy=2.7404844760894775, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/10_Step-6209.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 10
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_10.pb
Best checkpoint number: 4, Last checkpoint number: 8
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'7'}
Training> Name=main_level/agent, Worker=0, Episode=201, Total reward=0.33, Steps=6229, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=202, Total reward=0.22, Steps=6246, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=203, Total reward=0.1, Steps=6257, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=204, Total reward=27.15, Steps=6294, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=205, Total reward=27.68, Steps=6335, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=206, Total reward=11.13, Steps=6354, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=207, Total reward=8.84, Steps=6368, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=208, Total reward=0.12, Steps=6381, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=209, Total reward=2.77, Steps=6393, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=210, Total reward=0.4, Steps=6406, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=211, Total reward=1.75, Steps=6420, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=212, Total reward=4.15, Steps=6433, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=213, Total reward=4.53, Steps=6455, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=214, Total reward=0.12, Steps=6468, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=215, Total reward=0.3, Steps=6499, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=216, Total reward=0.2, Steps=6520, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=217, Total reward=0.24, Steps=6545, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=218, Total reward=0.36, Steps=6582, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=219, Total reward=0.29, Steps=6612, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=220, Total reward=0.34, Steps=6647, Training iteration=10
Policy training> Surrogate loss=0.027193181216716766, KL divergence=0.0336008183658123, Entropy=2.738353967666626, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.0297462847083807, KL divergence=0.10970418900251389, Entropy=2.738398551940918, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.09707211703062057, KL divergence=0.1102716326713562, Entropy=2.7386996746063232, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.06739389151334763, KL divergence=0.15095464885234833, Entropy=2.7389211654663086, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.09094187617301941, KL divergence=0.16024720668792725, Entropy=2.739027738571167, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.08599469810724258, KL divergence=0.15811164677143097, Entropy=2.7388107776641846, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.09496799856424332, KL divergence=0.15688084065914154, Entropy=2.7382516860961914, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.11857519298791885, KL divergence=0.1599532812833786, Entropy=2.7375125885009766, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.07595501095056534, KL divergence=0.18284423649311066, Entropy=2.736532211303711, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.09220948070287704, KL divergence=0.17323140799999237, Entropy=2.7352025508880615, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/11_Step-6647.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 11
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_11.pb
Best checkpoint number: 4, Last checkpoint number: 9
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'8'}
Training> Name=main_level/agent, Worker=0, Episode=221, Total reward=0.18, Steps=6666, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=222, Total reward=28.43, Steps=6743, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=223, Total reward=0.15, Steps=6759, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=224, Total reward=45.95, Steps=6826, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=225, Total reward=19.63, Steps=6853, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=226, Total reward=26.62, Steps=6888, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=227, Total reward=12.12, Steps=6905, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=228, Total reward=0.29, Steps=6935, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=229, Total reward=8.59, Steps=6963, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=230, Total reward=4.71, Steps=6980, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=231, Total reward=16.14, Steps=7019, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=232, Total reward=16.98, Steps=7047, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=233, Total reward=4.63, Steps=7068, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=234, Total reward=0.13, Steps=7082, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=235, Total reward=0.26, Steps=7109, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=236, Total reward=0.31, Steps=7141, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=237, Total reward=0.51, Steps=7193, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=238, Total reward=0.39, Steps=7233, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=239, Total reward=0.24, Steps=7258, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=240, Total reward=0.29, Steps=7288, Training iteration=11
Policy training> Surrogate loss=0.03875507041811943, KL divergence=0.0510840117931366, Entropy=2.733684539794922, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.021095452830195427, KL divergence=0.10656235367059708, Entropy=2.7333171367645264, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.07576261460781097, KL divergence=0.12510988116264343, Entropy=2.7328007221221924, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.08756503462791443, KL divergence=0.16335612535476685, Entropy=2.7324087619781494, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.1110740527510643, KL divergence=0.1662396639585495, Entropy=2.732104778289795, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.11815756559371948, KL divergence=0.165379136800766, Entropy=2.7317843437194824, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.12156722694635391, KL divergence=0.18941311538219452, Entropy=2.731468677520752, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.12331042438745499, KL divergence=0.18243931233882904, Entropy=2.731187105178833, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.12664058804512024, KL divergence=0.19894550740718842, Entropy=2.730424404144287, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.12544092535972595, KL divergence=0.1911448836326599, Entropy=2.729340076446533, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/12_Step-7288.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 12
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_12.pb
Best checkpoint number: 4, Last checkpoint number: 10
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'9'}
Training> Name=main_level/agent, Worker=0, Episode=241, Total reward=0.29, Steps=7318, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=242, Total reward=0.15, Steps=7334, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=243, Total reward=0.12, Steps=7347, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=244, Total reward=6.71, Steps=7360, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=245, Total reward=10.39, Steps=7381, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=246, Total reward=18.75, Steps=7435, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=247, Total reward=9.68, Steps=7450, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=248, Total reward=0.16, Steps=7467, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=249, Total reward=6.05, Steps=7489, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=250, Total reward=8.31, Steps=7508, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=251, Total reward=8.04, Steps=7532, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=252, Total reward=10.38, Steps=7556, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=253, Total reward=5.89, Steps=7578, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=254, Total reward=0.12, Steps=7591, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=255, Total reward=0.19, Steps=7611, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=256, Total reward=0.19, Steps=7631, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=257, Total reward=0.29, Steps=7661, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=258, Total reward=0.29, Steps=7691, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=259, Total reward=0.48, Steps=7740, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=260, Total reward=0.16, Steps=7757, Training iteration=12
Policy training> Surrogate loss=0.01759585738182068, KL divergence=0.04108544439077377, Entropy=2.72808837890625, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.03352075815200806, KL divergence=0.07591892778873444, Entropy=2.726752519607544, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.06486625969409943, KL divergence=0.1100325733423233, Entropy=2.725168466567993, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.06843702495098114, KL divergence=0.1353878378868103, Entropy=2.723479747772217, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.08029898256063461, KL divergence=0.1357564479112625, Entropy=2.7214999198913574, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.10389619320631027, KL divergence=0.13635945320129395, Entropy=2.7192955017089844, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.08724261075258255, KL divergence=0.14370213449001312, Entropy=2.7169442176818848, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.09021680057048798, KL divergence=0.1811889261007309, Entropy=2.7146077156066895, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.08631628006696701, KL divergence=0.15976309776306152, Entropy=2.711974620819092, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.1056019738316536, KL divergence=0.15514452755451202, Entropy=2.709146738052368, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/13_Step-7757.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 13
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_13.pb
Best checkpoint number: 4, Last checkpoint number: 11
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'10'}
Training> Name=main_level/agent, Worker=0, Episode=261, Total reward=5.71, Steps=7820, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=262, Total reward=0.55, Steps=7840, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=263, Total reward=0.4, Steps=7854, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=264, Total reward=11.61, Steps=7877, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=265, Total reward=12.36, Steps=7903, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=266, Total reward=15.83, Steps=7941, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=267, Total reward=6.64, Steps=7988, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=268, Total reward=0.13, Steps=8002, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=269, Total reward=10.5, Steps=8037, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=270, Total reward=9.36, Steps=8065, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=271, Total reward=13.11, Steps=8090, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=272, Total reward=19.24, Steps=8155, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=273, Total reward=6.17, Steps=8184, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=274, Total reward=0.3, Steps=8215, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=275, Total reward=0.23, Steps=8239, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=276, Total reward=0.25, Steps=8265, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=277, Total reward=0.23, Steps=8289, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=278, Total reward=0.28, Steps=8318, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=279, Total reward=0.32, Steps=8351, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=280, Total reward=0.36, Steps=8388, Training iteration=13
Policy training> Surrogate loss=0.009517848491668701, KL divergence=0.018871260806918144, Entropy=2.7060751914978027, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.060599762946367264, KL divergence=0.125766783952713, Entropy=2.703333854675293, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.07173189520835876, KL divergence=0.16779395937919617, Entropy=2.7022013664245605, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.056908003985881805, KL divergence=0.19867092370986938, Entropy=2.7020516395568848, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.05178739130496979, KL divergence=0.17194481194019318, Entropy=2.7015938758850098, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.09528079628944397, KL divergence=0.22580201923847198, Entropy=2.701124429702759, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.0831376165151596, KL divergence=0.21858355402946472, Entropy=2.7004776000976562, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.10111251473426819, KL divergence=0.217202290892601, Entropy=2.6996350288391113, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.11931402236223221, KL divergence=0.23358827829360962, Entropy=2.6987438201904297, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.11158730089664459, KL divergence=0.2345878779888153, Entropy=2.6975722312927246, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/14_Step-8388.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 14
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_14.pb
Best checkpoint number: 4, Last checkpoint number: 12
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'11'}
Training> Name=main_level/agent, Worker=0, Episode=281, Total reward=0.12, Steps=8401, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=282, Total reward=0.2, Steps=8422, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=283, Total reward=0.12, Steps=8435, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=284, Total reward=6.37, Steps=8450, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=285, Total reward=11.67, Steps=8473, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=286, Total reward=20.81, Steps=8502, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=287, Total reward=12.84, Steps=8522, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=288, Total reward=0.15, Steps=8538, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=289, Total reward=3.53, Steps=8553, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=290, Total reward=19.69, Steps=8618, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=291, Total reward=4.07, Steps=8639, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=292, Total reward=13.29, Steps=8664, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=293, Total reward=4.16, Steps=8692, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=294, Total reward=0.1, Steps=8703, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=295, Total reward=0.31, Steps=8735, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=296, Total reward=0.21, Steps=8757, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=297, Total reward=0.15, Steps=8773, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=298, Total reward=0.43, Steps=8817, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=299, Total reward=0.41, Steps=8859, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=300, Total reward=0.32, Steps=8892, Training iteration=14
Policy training> Surrogate loss=0.023613285273313522, KL divergence=0.023360325023531914, Entropy=2.697113513946533, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.06101272627711296, KL divergence=0.09517663717269897, Entropy=2.6983883380889893, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.07586915791034698, KL divergence=0.13141678273677826, Entropy=2.6997828483581543, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.06283379346132278, KL divergence=0.175941601395607, Entropy=2.7004733085632324, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.11424994468688965, KL divergence=0.22035200893878937, Entropy=2.7005181312561035, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.12611079216003418, KL divergence=0.20652179419994354, Entropy=2.700056314468384, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.1328306794166565, KL divergence=0.2142927497625351, Entropy=2.699176549911499, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.1226741224527359, KL divergence=0.24015387892723083, Entropy=2.6977264881134033, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.12452907115221024, KL divergence=0.2427782267332077, Entropy=2.6960747241973877, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.12816846370697021, KL divergence=0.20750753581523895, Entropy=2.694530725479126, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/15_Step-8892.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 15
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_15.pb
Best checkpoint number: 4, Last checkpoint number: 13
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'12'}
Training> Name=main_level/agent, Worker=0, Episode=301, Total reward=0.23, Steps=8916, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=302, Total reward=0.28, Steps=8945, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=303, Total reward=0.12, Steps=8958, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=304, Total reward=9.97, Steps=8974, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=305, Total reward=40.1, Steps=9084, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=306, Total reward=15.41, Steps=9113, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=307, Total reward=4.73, Steps=9164, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=308, Total reward=0.2, Steps=9174, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=309, Total reward=5.56, Steps=9196, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=310, Total reward=7.05, Steps=9214, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=311, Total reward=9.52, Steps=9235, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=312, Total reward=11.5, Steps=9257, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=313, Total reward=5.08, Steps=9273, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=314, Total reward=0.13, Steps=9287, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=315, Total reward=0.43, Steps=9331, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=316, Total reward=0.51, Steps=9383, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=317, Total reward=0.13, Steps=9397, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=318, Total reward=0.18, Steps=9416, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=319, Total reward=0.22, Steps=9439, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=320, Total reward=0.33, Steps=9473, Training iteration=15
Policy training> Surrogate loss=0.008719750680029392, KL divergence=0.022130446508526802, Entropy=2.6934056282043457, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.06433720886707306, KL divergence=0.068839430809021, Entropy=2.6934919357299805, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.08648283779621124, KL divergence=0.1706528663635254, Entropy=2.6926052570343018, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.10125499218702316, KL divergence=0.13685913383960724, Entropy=2.6913130283355713, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.11158820241689682, KL divergence=0.18213137984275818, Entropy=2.6899232864379883, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.1176578626036644, KL divergence=0.2058352530002594, Entropy=2.6878955364227295, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.11980394273996353, KL divergence=0.1977522075176239, Entropy=2.685346841812134, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.11032769083976746, KL divergence=0.2035205066204071, Entropy=2.6823532581329346, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.12044879794120789, KL divergence=0.22916020452976227, Entropy=2.6795148849487305, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.12546928226947784, KL divergence=0.23133212327957153, Entropy=2.6766626834869385, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/16_Step-9473.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 16
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_16.pb
Best checkpoint number: 4, Last checkpoint number: 14
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'13'}
Training> Name=main_level/agent, Worker=0, Episode=321, Total reward=0.08, Steps=9482, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=322, Total reward=0.16, Steps=9499, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=323, Total reward=0.42, Steps=9512, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=324, Total reward=16.4, Steps=9536, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=325, Total reward=27.83, Steps=9572, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=326, Total reward=26.66, Steps=9610, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=327, Total reward=9.07, Steps=9632, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=328, Total reward=0.18, Steps=9651, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=329, Total reward=7.04, Steps=9670, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=330, Total reward=9.0, Steps=9692, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=331, Total reward=17.12, Steps=9723, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=332, Total reward=13.83, Steps=9745, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=333, Total reward=4.03, Steps=9763, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=334, Total reward=0.1, Steps=9774, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=335, Total reward=0.18, Steps=9793, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=336, Total reward=0.17, Steps=9811, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=337, Total reward=0.37, Steps=9849, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=338, Total reward=0.23, Steps=9873, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=339, Total reward=0.31, Steps=9905, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=340, Total reward=0.11, Steps=9917, Training iteration=16
Policy training> Surrogate loss=0.009706893004477024, KL divergence=0.01762927509844303, Entropy=2.6746082305908203, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.07568433880805969, KL divergence=0.06373345106840134, Entropy=2.6745223999023438, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.08982212096452713, KL divergence=0.11317489296197891, Entropy=2.674753427505493, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.06556940823793411, KL divergence=0.14392386376857758, Entropy=2.6748580932617188, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.08975857496261597, KL divergence=0.17635659873485565, Entropy=2.6749773025512695, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.1302899420261383, KL divergence=0.21126006543636322, Entropy=2.6749277114868164, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.09812009334564209, KL divergence=0.2305433750152588, Entropy=2.6747970581054688, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.12228527665138245, KL divergence=0.20386947691440582, Entropy=2.674131155014038, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.13829317688941956, KL divergence=0.282564252614975, Entropy=2.6732728481292725, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.10443103313446045, KL divergence=0.254131555557251, Entropy=2.6721556186676025, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/17_Step-9917.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 17
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_17.pb
Best checkpoint number: 4, Last checkpoint number: 15
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'14'}
Training> Name=main_level/agent, Worker=0, Episode=341, Total reward=1.67, Steps=9964, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=342, Total reward=0.24, Steps=9989, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=343, Total reward=0.14, Steps=10004, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=344, Total reward=28.83, Steps=10047, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=345, Total reward=8.72, Steps=10066, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=346, Total reward=15.99, Steps=10092, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=347, Total reward=15.19, Steps=10113, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=348, Total reward=0.21, Steps=10135, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=349, Total reward=5.51, Steps=10158, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=350, Total reward=8.98, Steps=10181, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=351, Total reward=29.24, Steps=10233, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=352, Total reward=16.85, Steps=10284, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=353, Total reward=5.67, Steps=10310, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=354, Total reward=0.13, Steps=10324, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=355, Total reward=0.21, Steps=10346, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=356, Total reward=0.18, Steps=10365, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=357, Total reward=0.13, Steps=10379, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=358, Total reward=0.37, Steps=10417, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=359, Total reward=0.33, Steps=10451, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=360, Total reward=0.38, Steps=10490, Training iteration=17
Policy training> Surrogate loss=-0.010606173425912857, KL divergence=0.020443318411707878, Entropy=2.6713473796844482, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.039822980761528015, KL divergence=0.06580657511949539, Entropy=2.6711461544036865, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.05026857927441597, KL divergence=0.09147521108388901, Entropy=2.670638084411621, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.08059588074684143, KL divergence=0.11514044553041458, Entropy=2.67031192779541, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.11129511892795563, KL divergence=0.12990523874759674, Entropy=2.669698715209961, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.09653738886117935, KL divergence=0.14467483758926392, Entropy=2.6685373783111572, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.1267697960138321, KL divergence=0.1475987434387207, Entropy=2.666792392730713, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.09843608736991882, KL divergence=0.1621628701686859, Entropy=2.6648523807525635, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.098308265209198, KL divergence=0.16223153471946716, Entropy=2.6632065773010254, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.10177875310182571, KL divergence=0.16435255110263824, Entropy=2.6617352962493896, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/18_Step-10490.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 18
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_18.pb
Best checkpoint number: 4, Last checkpoint number: 16
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'15'}
Training> Name=main_level/agent, Worker=0, Episode=361, Total reward=0.2, Steps=10511, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=362, Total reward=0.25, Steps=10537, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=363, Total reward=0.1, Steps=10548, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=364, Total reward=16.58, Steps=10579, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=365, Total reward=21.93, Steps=10615, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=366, Total reward=8.85, Steps=10635, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=367, Total reward=9.59, Steps=10649, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=368, Total reward=0.22, Steps=10672, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=369, Total reward=0.15, Steps=10688, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=370, Total reward=7.17, Steps=10709, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=371, Total reward=29.66, Steps=10774, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=372, Total reward=7.25, Steps=10793, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=373, Total reward=5.42, Steps=10822, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=374, Total reward=0.13, Steps=10836, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=375, Total reward=0.27, Steps=10864, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=376, Total reward=0.17, Steps=10882, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=377, Total reward=0.09, Steps=10892, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=378, Total reward=0.26, Steps=10919, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=379, Total reward=0.46, Steps=10966, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=380, Total reward=0.3, Steps=10997, Training iteration=18
Policy training> Surrogate loss=0.04553950950503349, KL divergence=0.018534919247031212, Entropy=2.6600139141082764, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.016660187393426895, KL divergence=0.09973041713237762, Entropy=2.6580164432525635, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.08195938169956207, KL divergence=0.15610456466674805, Entropy=2.6571972370147705, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.05756146460771561, KL divergence=0.19745711982250214, Entropy=2.656348705291748, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.07363621890544891, KL divergence=0.214994415640831, Entropy=2.6552698612213135, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.09382171928882599, KL divergence=0.22455750405788422, Entropy=2.6543285846710205, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.10668276995420456, KL divergence=0.23749589920043945, Entropy=2.6532554626464844, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.07859797030687332, KL divergence=0.25186687707901, Entropy=2.652147054672241, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.09527252614498138, KL divergence=0.22229821979999542, Entropy=2.6509652137756348, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.09501547366380692, KL divergence=0.24357011914253235, Entropy=2.6496429443359375, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/19_Step-10997.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 19
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_19.pb
Best checkpoint number: 4, Last checkpoint number: 17
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'16'}
Training> Name=main_level/agent, Worker=0, Episode=381, Total reward=0.21, Steps=11019, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=382, Total reward=0.18, Steps=11038, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=383, Total reward=0.12, Steps=11051, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=384, Total reward=36.13, Steps=11124, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=385, Total reward=29.42, Steps=11172, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=386, Total reward=6.96, Steps=11189, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=387, Total reward=5.79, Steps=11212, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=388, Total reward=0.14, Steps=11226, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=389, Total reward=8.4, Steps=11249, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=390, Total reward=11.3, Steps=11280, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=391, Total reward=17.0, Steps=11316, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=392, Total reward=14.2, Steps=11357, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=393, Total reward=5.56, Steps=11380, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=394, Total reward=0.12, Steps=11393, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=395, Total reward=0.32, Steps=11426, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=396, Total reward=0.34, Steps=11461, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=397, Total reward=0.41, Steps=11503, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=398, Total reward=0.47, Steps=11551, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=399, Total reward=0.25, Steps=11577, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=400, Total reward=0.2, Steps=11598, Training iteration=19
Policy training> Surrogate loss=0.01538667269051075, KL divergence=0.05173219367861748, Entropy=2.6479620933532715, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.022243496030569077, KL divergence=0.12169991433620453, Entropy=2.647326946258545, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.07424578070640564, KL divergence=0.12162674963474274, Entropy=2.647310256958008, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.07438548654317856, KL divergence=0.14174392819404602, Entropy=2.646994113922119, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.10341581702232361, KL divergence=0.1683277189731598, Entropy=2.6465508937835693, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.10459809750318527, KL divergence=0.1930665820837021, Entropy=2.645777702331543, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.10289481282234192, KL divergence=0.1862519383430481, Entropy=2.644253730773926, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.11290803551673889, KL divergence=0.1853867918252945, Entropy=2.6421589851379395, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.11717632412910461, KL divergence=0.1902368813753128, Entropy=2.640138864517212, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.08624960482120514, KL divergence=0.20736105740070343, Entropy=2.637864112854004, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/20_Step-11598.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 20
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_20.pb
Best checkpoint number: 4, Last checkpoint number: 18
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'17'}
Training> Name=main_level/agent, Worker=0, Episode=401, Total reward=10.48, Steps=11651, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=402, Total reward=3.1, Steps=11687, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=403, Total reward=0.1, Steps=11698, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=404, Total reward=11.93, Steps=11720, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=405, Total reward=45.82, Steps=11788, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=406, Total reward=17.36, Steps=11827, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=407, Total reward=7.79, Steps=11839, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=408, Total reward=0.12, Steps=11852, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=409, Total reward=4.93, Steps=11869, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=410, Total reward=12.77, Steps=11917, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=411, Total reward=12.02, Steps=11954, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=412, Total reward=17.1, Steps=11985, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=413, Total reward=4.47, Steps=12001, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=414, Total reward=0.1, Steps=12012, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=415, Total reward=0.18, Steps=12031, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=416, Total reward=0.18, Steps=12050, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=417, Total reward=0.43, Steps=12094, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=418, Total reward=0.49, Steps=12144, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=419, Total reward=0.18, Steps=12163, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=420, Total reward=0.16, Steps=12180, Training iteration=20
Policy training> Surrogate loss=0.002133258618414402, KL divergence=0.024000760167837143, Entropy=2.63581919670105, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.06539914011955261, KL divergence=0.11738189309835434, Entropy=2.634676456451416, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.09437102824449539, KL divergence=0.13886255025863647, Entropy=2.6343271732330322, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.10210837423801422, KL divergence=0.17153941094875336, Entropy=2.6332895755767822, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.10692662745714188, KL divergence=0.16615764796733856, Entropy=2.6316473484039307, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.1140303760766983, KL divergence=0.17157624661922455, Entropy=2.629964828491211, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.10775041580200195, KL divergence=0.17877207696437836, Entropy=2.628203868865967, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.11929448693990707, KL divergence=0.17148849368095398, Entropy=2.6263070106506348, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.1218319833278656, KL divergence=0.1929502785205841, Entropy=2.624389886856079, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.11541867256164551, KL divergence=0.20051611959934235, Entropy=2.6223812103271484, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/21_Step-12180.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 21
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_21.pb
Best checkpoint number: 4, Last checkpoint number: 19
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'18'}
Training> Name=main_level/agent, Worker=0, Episode=421, Total reward=0.44, Steps=12225, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=422, Total reward=0.34, Steps=12243, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=423, Total reward=0.11, Steps=12255, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=424, Total reward=2.06, Steps=12262, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=425, Total reward=7.93, Steps=12278, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=426, Total reward=21.87, Steps=12313, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=427, Total reward=8.85, Steps=12326, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=428, Total reward=0.15, Steps=12342, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=429, Total reward=4.34, Steps=12386, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=430, Total reward=8.74, Steps=12413, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=431, Total reward=27.2, Steps=12465, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=432, Total reward=8.46, Steps=12481, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=433, Total reward=5.15, Steps=12509, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=434, Total reward=0.11, Steps=12521, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=435, Total reward=0.18, Steps=12540, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=436, Total reward=0.26, Steps=12567, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=437, Total reward=0.38, Steps=12606, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=438, Total reward=0.35, Steps=12642, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=439, Total reward=1.28, Steps=12705, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=440, Total reward=0.21, Steps=12727, Training iteration=21
Policy training> Surrogate loss=0.016841208562254906, KL divergence=0.026591651141643524, Entropy=2.6210851669311523, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.03730575367808342, KL divergence=0.13093945384025574, Entropy=2.620352268218994, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.0628446564078331, KL divergence=0.191778302192688, Entropy=2.618032932281494, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.09225000441074371, KL divergence=0.18969398736953735, Entropy=2.6161813735961914, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.0811866968870163, KL divergence=0.22900119423866272, Entropy=2.6144769191741943, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.08168277889490128, KL divergence=0.21201655268669128, Entropy=2.6129870414733887, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.12444473803043365, KL divergence=0.21764662861824036, Entropy=2.6117019653320312, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.09742877632379532, KL divergence=0.23044630885124207, Entropy=2.6102347373962402, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.08729042857885361, KL divergence=0.23008078336715698, Entropy=2.60880970954895, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.10321241617202759, KL divergence=0.22882193326950073, Entropy=2.607495069503784, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/22_Step-12727.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 22
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_22.pb
Best checkpoint number: 4, Last checkpoint number: 20
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'19'}
Training> Name=main_level/agent, Worker=0, Episode=441, Total reward=0.11, Steps=12739, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=442, Total reward=0.16, Steps=12756, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=443, Total reward=0.11, Steps=12768, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=444, Total reward=29.83, Steps=12814, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=445, Total reward=7.75, Steps=12830, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=446, Total reward=9.44, Steps=12846, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=447, Total reward=16.49, Steps=12870, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=448, Total reward=0.36, Steps=12907, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=449, Total reward=3.71, Steps=12951, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=450, Total reward=4.52, Steps=12983, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=451, Total reward=14.63, Steps=13018, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=452, Total reward=13.42, Steps=13043, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=453, Total reward=4.86, Steps=13080, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=454, Total reward=0.1, Steps=13091, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=455, Total reward=0.25, Steps=13117, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=456, Total reward=0.24, Steps=13142, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=457, Total reward=0.38, Steps=13181, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=458, Total reward=0.73, Steps=13255, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=459, Total reward=0.15, Steps=13271, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=460, Total reward=0.4, Steps=13312, Training iteration=22
Policy training> Surrogate loss=0.013066079467535019, KL divergence=0.0187979768961668, Entropy=2.60650897026062, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.026150966063141823, KL divergence=0.16623295843601227, Entropy=2.6086196899414062, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.055810730904340744, KL divergence=0.19473299384117126, Entropy=2.611680269241333, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.0748462975025177, KL divergence=0.2277822196483612, Entropy=2.6137778759002686, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.08993829786777496, KL divergence=0.21023587882518768, Entropy=2.6142899990081787, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.0904446393251419, KL divergence=0.31363698840141296, Entropy=2.6139261722564697, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.10558050125837326, KL divergence=0.23889034986495972, Entropy=2.612917184829712, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.10323270410299301, KL divergence=0.25047922134399414, Entropy=2.611616849899292, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.09951174259185791, KL divergence=0.2889096140861511, Entropy=2.6096434593200684, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.09919925034046173, KL divergence=0.2685747742652893, Entropy=2.6077795028686523, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/23_Step-13312.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 23
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_23.pb
Best checkpoint number: 4, Last checkpoint number: 21
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'20'}
Training> Name=main_level/agent, Worker=0, Episode=461, Total reward=0.23, Steps=13336, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=462, Total reward=0.22, Steps=13355, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=463, Total reward=0.12, Steps=13368, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=464, Total reward=13.44, Steps=13391, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=465, Total reward=37.98, Steps=13450, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=466, Total reward=21.12, Steps=13489, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=467, Total reward=14.73, Steps=13514, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=468, Total reward=0.13, Steps=13528, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=469, Total reward=4.17, Steps=13553, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=470, Total reward=5.35, Steps=13574, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=471, Total reward=10.41, Steps=13594, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=472, Total reward=5.35, Steps=13608, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=473, Total reward=3.45, Steps=13627, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=474, Total reward=0.11, Steps=13639, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=475, Total reward=0.21, Steps=13661, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=476, Total reward=0.29, Steps=13691, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=477, Total reward=0.17, Steps=13709, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=478, Total reward=0.63, Steps=13773, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=479, Total reward=0.26, Steps=13800, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=480, Total reward=0.21, Steps=13822, Training iteration=23
Policy training> Surrogate loss=0.03682851046323776, KL divergence=0.019963338971138, Entropy=2.6072192192077637, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.07253731787204742, KL divergence=0.06952277570962906, Entropy=2.607070207595825, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.051188211888074875, KL divergence=0.13468943536281586, Entropy=2.6062684059143066, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.11301569640636444, KL divergence=0.15517260134220123, Entropy=2.605117082595825, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.11445087194442749, KL divergence=0.18122532963752747, Entropy=2.603454113006592, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.08114773035049438, KL divergence=0.25110945105552673, Entropy=2.6012845039367676, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.10497675836086273, KL divergence=0.19206596910953522, Entropy=2.5993237495422363, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.07046501338481903, KL divergence=0.21791693568229675, Entropy=2.5972135066986084, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.1340080201625824, KL divergence=0.25084641575813293, Entropy=2.5952823162078857, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.13274849951267242, KL divergence=0.22964458167552948, Entropy=2.593510150909424, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/24_Step-13822.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 24
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_24.pb
Best checkpoint number: 4, Last checkpoint number: 22
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'21'}
Training> Name=main_level/agent, Worker=0, Episode=481, Total reward=0.23, Steps=13846, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=482, Total reward=0.27, Steps=13874, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=483, Total reward=0.15, Steps=13890, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=484, Total reward=18.1, Steps=13920, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=485, Total reward=27.11, Steps=13963, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=486, Total reward=17.14, Steps=13990, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=487, Total reward=11.0, Steps=14007, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=488, Total reward=0.35, Steps=14024, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=489, Total reward=6.05, Steps=14043, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=490, Total reward=9.63, Steps=14071, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=491, Total reward=12.7, Steps=14106, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=492, Total reward=3.24, Steps=14119, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=493, Total reward=5.8, Steps=14154, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=494, Total reward=0.12, Steps=14167, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=495, Total reward=0.44, Steps=14212, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=496, Total reward=0.27, Steps=14240, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=497, Total reward=0.52, Steps=14293, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=498, Total reward=0.35, Steps=14329, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=499, Total reward=0.38, Steps=14368, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=500, Total reward=0.24, Steps=14393, Training iteration=24
Policy training> Surrogate loss=-0.006013783626258373, KL divergence=0.01073683612048626, Entropy=2.591435670852661, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.004492303356528282, KL divergence=0.08414562046527863, Entropy=2.590298891067505, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.0703386664390564, KL divergence=0.18061323463916779, Entropy=2.590336561203003, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.06248047947883606, KL divergence=0.18449969589710236, Entropy=2.5901169776916504, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.07962016761302948, KL divergence=0.1881686896085739, Entropy=2.5892937183380127, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.07356593012809753, KL divergence=0.1994529813528061, Entropy=2.5882821083068848, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.09697592258453369, KL divergence=0.1987285614013672, Entropy=2.587196111679077, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.07765640318393707, KL divergence=0.2019580900669098, Entropy=2.586056709289551, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.07088058441877365, KL divergence=0.20491810142993927, Entropy=2.585198402404785, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.08490218222141266, KL divergence=0.21077114343643188, Entropy=2.584028959274292, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/25_Step-14393.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 25
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_25.pb
Best checkpoint number: 4, Last checkpoint number: 23
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'22'}
Training> Name=main_level/agent, Worker=0, Episode=501, Total reward=0.22, Steps=14416, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=502, Total reward=0.32, Steps=14449, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=503, Total reward=0.17, Steps=14467, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=504, Total reward=20.06, Steps=14502, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=505, Total reward=19.46, Steps=14536, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=506, Total reward=11.85, Steps=14558, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=507, Total reward=11.62, Steps=14584, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=508, Total reward=0.3, Steps=14615, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=509, Total reward=8.58, Steps=14641, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=510, Total reward=11.05, Steps=14673, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=511, Total reward=23.21, Steps=14723, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=512, Total reward=21.47, Steps=14778, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=513, Total reward=7.14, Steps=14826, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=514, Total reward=0.13, Steps=14840, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=515, Total reward=0.22, Steps=14863, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=516, Total reward=0.19, Steps=14883, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=517, Total reward=0.17, Steps=14901, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=518, Total reward=0.44, Steps=14946, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=519, Total reward=0.32, Steps=14979, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=520, Total reward=0.18, Steps=14998, Training iteration=25
Policy training> Surrogate loss=-0.0003815781674347818, KL divergence=0.012968725524842739, Entropy=2.5823309421539307, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.05095367506146431, KL divergence=0.08200035244226456, Entropy=2.5797274112701416, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.06658945232629776, KL divergence=0.13424843549728394, Entropy=2.5773370265960693, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.08718124032020569, KL divergence=0.1701396405696869, Entropy=2.575279474258423, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.09604015946388245, KL divergence=0.15067808330059052, Entropy=2.573626756668091, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.09976106137037277, KL divergence=0.17123956978321075, Entropy=2.571493148803711, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.10293387621641159, KL divergence=0.17181074619293213, Entropy=2.5690927505493164, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.10454990714788437, KL divergence=0.1875552535057068, Entropy=2.567060947418213, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.10966576635837555, KL divergence=0.19181105494499207, Entropy=2.564527750015259, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.1208445206284523, KL divergence=0.18931694328784943, Entropy=2.5616064071655273, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/26_Step-14998.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 26
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_26.pb
Best checkpoint number: 4, Last checkpoint number: 24
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'23'}
Training> Name=main_level/agent, Worker=0, Episode=521, Total reward=0.29, Steps=15028, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=522, Total reward=0.86, Steps=15049, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=523, Total reward=1.47, Steps=15069, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=524, Total reward=10.53, Steps=15089, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=525, Total reward=18.68, Steps=15121, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=526, Total reward=25.36, Steps=15156, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=527, Total reward=16.09, Steps=15180, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=528, Total reward=0.2, Steps=15201, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=529, Total reward=15.03, Steps=15265, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=530, Total reward=10.35, Steps=15291, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=531, Total reward=7.84, Steps=15324, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=532, Total reward=7.18, Steps=15347, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=533, Total reward=7.42, Steps=15393, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=534, Total reward=0.18, Steps=15412, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=535, Total reward=0.41, Steps=15454, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=536, Total reward=0.33, Steps=15488, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=537, Total reward=0.23, Steps=15512, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=538, Total reward=0.24, Steps=15537, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=539, Total reward=0.21, Steps=15559, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=540, Total reward=0.52, Steps=15612, Training iteration=26
Policy training> Surrogate loss=0.009214848279953003, KL divergence=0.012159557081758976, Entropy=2.5596678256988525, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.0685986801981926, KL divergence=0.11584672331809998, Entropy=2.5592353343963623, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.07962033152580261, KL divergence=0.2009541243314743, Entropy=2.557990789413452, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.0721386969089508, KL divergence=0.21980559825897217, Entropy=2.5569169521331787, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.10087721049785614, KL divergence=0.21671929955482483, Entropy=2.5555617809295654, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.10528111457824707, KL divergence=0.1970859169960022, Entropy=2.553812265396118, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.10937417298555374, KL divergence=0.22225263714790344, Entropy=2.5525479316711426, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.10062718391418457, KL divergence=0.19754129648208618, Entropy=2.551558017730713, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.11214545369148254, KL divergence=0.206131249666214, Entropy=2.5507774353027344, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.09188695251941681, KL divergence=0.21462199091911316, Entropy=2.549886465072632, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/27_Step-15612.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 27
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_27.pb
Best checkpoint number: 4, Last checkpoint number: 25
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'24'}
Training> Name=main_level/agent, Worker=0, Episode=541, Total reward=0.21, Steps=15634, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=542, Total reward=3.89, Steps=15677, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=543, Total reward=0.14, Steps=15692, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=544, Total reward=16.43, Steps=15719, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=545, Total reward=38.35, Steps=15779, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=546, Total reward=25.61, Steps=15818, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=547, Total reward=18.42, Steps=15844, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=548, Total reward=0.67, Steps=15862, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=549, Total reward=5.77, Steps=15879, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=550, Total reward=9.55, Steps=15905, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=551, Total reward=5.88, Steps=15928, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=552, Total reward=15.12, Steps=15968, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=553, Total reward=8.01, Steps=15993, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=554, Total reward=0.13, Steps=16007, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=555, Total reward=0.58, Steps=16066, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=556, Total reward=0.28, Steps=16095, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=557, Total reward=0.14, Steps=16110, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=558, Total reward=0.53, Steps=16164, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=559, Total reward=0.21, Steps=16186, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=560, Total reward=0.22, Steps=16209, Training iteration=27
Policy training> Surrogate loss=0.011471264064311981, KL divergence=0.01270245760679245, Entropy=2.549870491027832, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.06320611387491226, KL divergence=0.07532449811697006, Entropy=2.5508363246917725, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.08218061923980713, KL divergence=0.13423006236553192, Entropy=2.5514631271362305, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.09027732163667679, KL divergence=0.15795327723026276, Entropy=2.5519185066223145, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.10400005429983139, KL divergence=0.1944330781698227, Entropy=2.551938772201538, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.1125236228108406, KL divergence=0.21583788096904755, Entropy=2.552417278289795, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.1103985384106636, KL divergence=0.24191978573799133, Entropy=2.5525245666503906, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.11619630455970764, KL divergence=0.24398596584796906, Entropy=2.5519113540649414, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.10360938310623169, KL divergence=0.25328728556632996, Entropy=2.551358461380005, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.11881029605865479, KL divergence=0.2579908072948456, Entropy=2.550746440887451, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/28_Step-16209.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 28
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_28.pb
Best checkpoint number: 4, Last checkpoint number: 26
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'25'}
Training> Name=main_level/agent, Worker=0, Episode=561, Total reward=0.24, Steps=16234, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=562, Total reward=1.2, Steps=16257, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=563, Total reward=0.16, Steps=16274, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=564, Total reward=18.54, Steps=16307, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=565, Total reward=51.18, Steps=16382, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=566, Total reward=20.73, Steps=16420, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=567, Total reward=16.39, Steps=16442, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=568, Total reward=0.18, Steps=16461, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=569, Total reward=6.51, Steps=16486, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=570, Total reward=12.85, Steps=16513, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=571, Total reward=17.03, Steps=16547, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=572, Total reward=8.35, Steps=16574, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=573, Total reward=4.55, Steps=16596, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=574, Total reward=0.17, Steps=16614, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=575, Total reward=0.53, Steps=16668, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=576, Total reward=0.32, Steps=16701, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=577, Total reward=0.17, Steps=16719, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=578, Total reward=0.25, Steps=16745, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=579, Total reward=0.49, Steps=16795, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=580, Total reward=0.29, Steps=16825, Training iteration=28
Policy training> Surrogate loss=-0.008818287402391434, KL divergence=0.02887987159192562, Entropy=2.550532579421997, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.04914052411913872, KL divergence=0.09648448973894119, Entropy=2.5518839359283447, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.08096751570701599, KL divergence=0.19080106914043427, Entropy=2.5521979331970215, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.0721266120672226, KL divergence=0.18660688400268555, Entropy=2.5526394844055176, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.09312469512224197, KL divergence=0.22693294286727905, Entropy=2.55304217338562, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.09803389757871628, KL divergence=0.28220975399017334, Entropy=2.553147792816162, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.10933932662010193, KL divergence=0.23710209131240845, Entropy=2.5524351596832275, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.1263921558856964, KL divergence=0.30988553166389465, Entropy=2.5513010025024414, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.11604531854391098, KL divergence=0.2940022647380829, Entropy=2.5500895977020264, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.11719629168510437, KL divergence=0.32442203164100647, Entropy=2.5485830307006836, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/29_Step-16825.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 29
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_29.pb
Best checkpoint number: 4, Last checkpoint number: 27
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'26'}
Training> Name=main_level/agent, Worker=0, Episode=581, Total reward=0.87, Steps=16880, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=582, Total reward=0.24, Steps=16905, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=583, Total reward=0.11, Steps=16917, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=584, Total reward=8.37, Steps=16932, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=585, Total reward=35.87, Steps=16980, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=586, Total reward=6.96, Steps=16996, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=587, Total reward=16.26, Steps=17019, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=588, Total reward=0.16, Steps=17036, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=589, Total reward=0.15, Steps=17052, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=590, Total reward=7.26, Steps=17069, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=591, Total reward=26.63, Steps=17129, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=592, Total reward=4.35, Steps=17142, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=593, Total reward=3.96, Steps=17168, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=594, Total reward=0.11, Steps=17180, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=595, Total reward=0.19, Steps=17200, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=596, Total reward=0.25, Steps=17226, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=597, Total reward=0.2, Steps=17247, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=598, Total reward=0.33, Steps=17281, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=599, Total reward=0.3, Steps=17312, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=600, Total reward=0.15, Steps=17328, Training iteration=29
Policy training> Surrogate loss=0.03409230709075928, KL divergence=0.013126826845109463, Entropy=2.5474679470062256, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.04080084338784218, KL divergence=0.14482653141021729, Entropy=2.546832799911499, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.08913139253854752, KL divergence=0.18599878251552582, Entropy=2.5460853576660156, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.09903059899806976, KL divergence=0.24333153665065765, Entropy=2.5448145866394043, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.10220254212617874, KL divergence=0.24899323284626007, Entropy=2.543513059616089, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.0775824785232544, KL divergence=0.2996388375759125, Entropy=2.542440891265869, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.0889580175280571, KL divergence=0.27482229471206665, Entropy=2.541151762008667, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.06053401902318001, KL divergence=0.26470592617988586, Entropy=2.5396840572357178, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.09338383376598358, KL divergence=0.24833334982395172, Entropy=2.5383799076080322, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.11719853430986404, KL divergence=0.25598159432411194, Entropy=2.5373618602752686, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/30_Step-17328.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 30
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_30.pb
Best checkpoint number: 4, Last checkpoint number: 28
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'27'}
Training> Name=main_level/agent, Worker=0, Episode=601, Total reward=0.44, Steps=17366, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=602, Total reward=0.8, Steps=17381, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=603, Total reward=0.09, Steps=17391, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=604, Total reward=7.03, Steps=17407, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=605, Total reward=15.01, Steps=17436, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=606, Total reward=16.33, Steps=17468, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=607, Total reward=11.32, Steps=17487, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=608, Total reward=0.32, Steps=17520, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=609, Total reward=6.06, Steps=17550, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=610, Total reward=1.07, Steps=17580, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=611, Total reward=6.41, Steps=17604, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=612, Total reward=13.73, Steps=17632, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=613, Total reward=5.79, Steps=17650, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=614, Total reward=0.09, Steps=17660, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=615, Total reward=0.2, Steps=17681, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=616, Total reward=0.38, Steps=17720, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=617, Total reward=0.16, Steps=17737, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=618, Total reward=0.53, Steps=17791, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=619, Total reward=0.22, Steps=17814, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=620, Total reward=0.33, Steps=17848, Training iteration=30
Policy training> Surrogate loss=0.008963681757450104, KL divergence=0.019082849845290184, Entropy=2.5365967750549316, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.055853221565485, KL divergence=0.13095711171627045, Entropy=2.536670446395874, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.0697445273399353, KL divergence=0.21037021279335022, Entropy=2.5368690490722656, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.08469652384519577, KL divergence=0.2550230324268341, Entropy=2.5362930297851562, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.0892043486237526, KL divergence=0.24468007683753967, Entropy=2.5346391201019287, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.1008833572268486, KL divergence=0.2771604657173157, Entropy=2.5325798988342285, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.0963040292263031, KL divergence=0.26150965690612793, Entropy=2.5302529335021973, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.08531204611063004, KL divergence=0.23837661743164062, Entropy=2.5277748107910156, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.09855486452579498, KL divergence=0.2658277153968811, Entropy=2.5253725051879883, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.10101925581693649, KL divergence=0.23993241786956787, Entropy=2.5229125022888184, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/31_Step-17848.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 31
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_31.pb
Best checkpoint number: 4, Last checkpoint number: 29
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'28'}
Training> Name=main_level/agent, Worker=0, Episode=621, Total reward=0.23, Steps=17872, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=622, Total reward=0.16, Steps=17888, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=623, Total reward=0.1, Steps=17899, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=624, Total reward=5.44, Steps=17912, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=625, Total reward=36.77, Steps=17970, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=626, Total reward=9.65, Steps=17987, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=627, Total reward=10.35, Steps=18002, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=628, Total reward=0.33, Steps=18036, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=629, Total reward=4.85, Steps=18055, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=630, Total reward=8.28, Steps=18099, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=631, Total reward=12.29, Steps=18120, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=632, Total reward=19.35, Steps=18174, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=633, Total reward=5.86, Steps=18207, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=634, Total reward=0.1, Steps=18218, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=635, Total reward=0.28, Steps=18247, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=636, Total reward=0.44, Steps=18292, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=637, Total reward=0.16, Steps=18309, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=638, Total reward=0.24, Steps=18334, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=639, Total reward=0.32, Steps=18367, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=640, Total reward=0.39, Steps=18407, Training iteration=31
Policy training> Surrogate loss=0.011216957122087479, KL divergence=0.02353200688958168, Entropy=2.5213756561279297, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.027588292956352234, KL divergence=0.10011108219623566, Entropy=2.522045612335205, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.0647929459810257, KL divergence=0.20462992787361145, Entropy=2.5227532386779785, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.0836310163140297, KL divergence=0.22119960188865662, Entropy=2.523312568664551, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.11546985059976578, KL divergence=0.210662841796875, Entropy=2.5237743854522705, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.1192217543721199, KL divergence=0.2647038698196411, Entropy=2.5234084129333496, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.10875783860683441, KL divergence=0.21585282683372498, Entropy=2.522824764251709, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.12128587067127228, KL divergence=0.23103944957256317, Entropy=2.522291660308838, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.07896579802036285, KL divergence=0.24420015513896942, Entropy=2.5216219425201416, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.10669837892055511, KL divergence=0.24068313837051392, Entropy=2.520651340484619, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/32_Step-18407.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 32
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_32.pb
Best checkpoint number: 4, Last checkpoint number: 30
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'29'}
Training> Name=main_level/agent, Worker=0, Episode=641, Total reward=0.23, Steps=18431, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=642, Total reward=0.44, Steps=18448, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=643, Total reward=0.11, Steps=18460, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=644, Total reward=16.63, Steps=18487, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=645, Total reward=14.63, Steps=18517, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=646, Total reward=29.28, Steps=18560, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=647, Total reward=13.05, Steps=18581, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=648, Total reward=0.28, Steps=18610, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=649, Total reward=8.5, Steps=18645, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=650, Total reward=15.78, Steps=18682, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=651, Total reward=25.93, Steps=18729, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=652, Total reward=6.11, Steps=18744, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=653, Total reward=4.71, Steps=18780, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=654, Total reward=0.12, Steps=18793, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=655, Total reward=0.34, Steps=18828, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=656, Total reward=0.48, Steps=18877, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=657, Total reward=0.17, Steps=18895, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=658, Total reward=0.23, Steps=18919, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=659, Total reward=0.47, Steps=18967, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=660, Total reward=0.19, Steps=18987, Training iteration=32
Policy training> Surrogate loss=0.011792674660682678, KL divergence=0.017317406833171844, Entropy=2.519894599914551, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.051124513149261475, KL divergence=0.12730012834072113, Entropy=2.521296501159668, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.0726943090558052, KL divergence=0.14543993771076202, Entropy=2.521641969680786, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.08475463837385178, KL divergence=0.22339794039726257, Entropy=2.5208373069763184, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.08938197046518326, KL divergence=0.22914902865886688, Entropy=2.5194616317749023, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.10600927472114563, KL divergence=0.2299032211303711, Entropy=2.517714500427246, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.1145595833659172, KL divergence=0.21121610701084137, Entropy=2.51572847366333, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.11618142575025558, KL divergence=0.23656590282917023, Entropy=2.5134310722351074, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.11722052097320557, KL divergence=0.2417667955160141, Entropy=2.510924816131592, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.11804451048374176, KL divergence=0.2622833847999573, Entropy=2.508389472961426, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/33_Step-18987.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 33
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_33.pb
Best checkpoint number: 4, Last checkpoint number: 31
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'30'}
Training> Name=main_level/agent, Worker=0, Episode=661, Total reward=0.2, Steps=19008, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=662, Total reward=0.62, Steps=19034, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=663, Total reward=0.16, Steps=19051, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=664, Total reward=31.0, Steps=19107, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=665, Total reward=11.73, Steps=19128, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=666, Total reward=40.8, Steps=19193, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=667, Total reward=18.84, Steps=19221, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=668, Total reward=0.25, Steps=19243, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=669, Total reward=9.1, Steps=19264, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=670, Total reward=4.93, Steps=19314, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=671, Total reward=29.42, Steps=19361, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=672, Total reward=30.83, Steps=19435, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=673, Total reward=10.1, Steps=19487, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=674, Total reward=0.16, Steps=19504, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=675, Total reward=0.55, Steps=19560, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=676, Total reward=0.56, Steps=19617, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=677, Total reward=0.32, Steps=19650, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=678, Total reward=0.34, Steps=19685, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=679, Total reward=0.39, Steps=19725, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=680, Total reward=0.24, Steps=19750, Training iteration=33
Policy training> Surrogate loss=0.006070889066904783, KL divergence=0.023927466943860054, Entropy=2.5052754878997803, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.05242440849542618, KL divergence=0.10120510309934616, Entropy=2.5024003982543945, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.09815464168787003, KL divergence=0.14714793860912323, Entropy=2.500765085220337, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.11499731987714767, KL divergence=0.17287935316562653, Entropy=2.4993574619293213, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.10201458632946014, KL divergence=0.1932307630777359, Entropy=2.4979259967803955, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.1210397481918335, KL divergence=0.19581986963748932, Entropy=2.495961904525757, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.14048713445663452, KL divergence=0.20600737631320953, Entropy=2.4932150840759277, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.11949408054351807, KL divergence=0.20254959166049957, Entropy=2.4895081520080566, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.1216171532869339, KL divergence=0.2059619426727295, Entropy=2.4864039421081543, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.12584112584590912, KL divergence=0.21717752516269684, Entropy=2.4829442501068115, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/34_Step-19750.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 34
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_34.pb
Best checkpoint number: 4, Last checkpoint number: 32
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'31'}
Training> Name=main_level/agent, Worker=0, Episode=681, Total reward=0.25, Steps=19776, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=682, Total reward=14.05, Steps=19831, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=683, Total reward=0.14, Steps=19846, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=684, Total reward=51.52, Steps=19922, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=685, Total reward=24.68, Steps=19963, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=686, Total reward=32.85, Steps=20014, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=687, Total reward=13.95, Steps=20042, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=688, Total reward=0.38, Steps=20054, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=689, Total reward=7.89, Steps=20082, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=690, Total reward=9.14, Steps=20110, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=691, Total reward=13.7, Steps=20134, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=692, Total reward=7.71, Steps=20154, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=693, Total reward=5.81, Steps=20195, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=694, Total reward=0.11, Steps=20207, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=695, Total reward=0.41, Steps=20249, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=696, Total reward=0.26, Steps=20276, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=697, Total reward=0.21, Steps=20298, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=698, Total reward=0.4, Steps=20339, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=699, Total reward=0.19, Steps=20359, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=700, Total reward=0.31, Steps=20391, Training iteration=34
Policy training> Surrogate loss=0.015978146344423294, KL divergence=0.021226871758699417, Entropy=2.4799671173095703, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.07402617484331131, KL divergence=0.12266208976507187, Entropy=2.477729558944702, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.08839033544063568, KL divergence=0.17870716750621796, Entropy=2.4759132862091064, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.11065784841775894, KL divergence=0.19221392273902893, Entropy=2.4743757247924805, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.11443576961755753, KL divergence=0.21031861007213593, Entropy=2.4718093872070312, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.12001464515924454, KL divergence=0.21259649097919464, Entropy=2.4687817096710205, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.11794421821832657, KL divergence=0.22314004600048065, Entropy=2.4654102325439453, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.12295504659414291, KL divergence=0.234409898519516, Entropy=2.4617128372192383, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.12068885564804077, KL divergence=0.246079683303833, Entropy=2.457559823989868, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.12271018326282501, KL divergence=0.24722877144813538, Entropy=2.4532036781311035, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/35_Step-20391.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 35
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_35.pb
Best checkpoint number: 4, Last checkpoint number: 33
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'32'}
Training> Name=main_level/agent, Worker=0, Episode=701, Total reward=14.9, Steps=20463, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=702, Total reward=1.04, Steps=20484, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=703, Total reward=0.9, Steps=20501, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=704, Total reward=44.84, Steps=20577, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=705, Total reward=24.28, Steps=20620, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=706, Total reward=24.29, Steps=20658, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=707, Total reward=15.4, Steps=20687, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=708, Total reward=0.16, Steps=20704, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=709, Total reward=8.33, Steps=20733, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=710, Total reward=9.65, Steps=20763, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=711, Total reward=26.82, Steps=20820, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=712, Total reward=11.99, Steps=20870, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=713, Total reward=6.2, Steps=20899, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=714, Total reward=0.13, Steps=20913, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=715, Total reward=0.24, Steps=20938, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=716, Total reward=0.17, Steps=20956, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=717, Total reward=0.24, Steps=20981, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=718, Total reward=0.6, Steps=21042, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=719, Total reward=0.54, Steps=21097, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=720, Total reward=0.25, Steps=21123, Training iteration=35
Policy training> Surrogate loss=0.029535112902522087, KL divergence=0.023773841559886932, Entropy=2.450611114501953, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.048884905874729156, KL divergence=0.1402406543493271, Entropy=2.4520158767700195, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.0781116783618927, KL divergence=0.17849677801132202, Entropy=2.452972173690796, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.10317221283912659, KL divergence=0.21593011915683746, Entropy=2.452125072479248, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.11153778433799744, KL divergence=0.18956080079078674, Entropy=2.450079917907715, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.1141667366027832, KL divergence=0.21520811319351196, Entropy=2.448241710662842, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.10744181275367737, KL divergence=0.20540064573287964, Entropy=2.446305751800537, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.11756572127342224, KL divergence=0.2219070941209793, Entropy=2.443941354751587, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.11407468467950821, KL divergence=0.21313892304897308, Entropy=2.4410669803619385, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.10636980086565018, KL divergence=0.2217065840959549, Entropy=2.4385223388671875, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/36_Step-21123.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 36
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_36.pb
Best checkpoint number: 4, Last checkpoint number: 34
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'33'}
Training> Name=main_level/agent, Worker=0, Episode=721, Total reward=0.24, Steps=21148, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=722, Total reward=9.28, Steps=21190, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=723, Total reward=1.28, Steps=21207, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=724, Total reward=13.92, Steps=21229, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=725, Total reward=44.28, Steps=21291, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=726, Total reward=31.35, Steps=21338, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=727, Total reward=3.23, Steps=21357, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=728, Total reward=0.16, Steps=21374, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=729, Total reward=6.06, Steps=21397, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=730, Total reward=24.06, Steps=21467, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=731, Total reward=26.25, Steps=21538, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=732, Total reward=5.98, Steps=21554, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=733, Total reward=5.45, Steps=21573, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=734, Total reward=0.1, Steps=21584, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=735, Total reward=0.26, Steps=21611, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=736, Total reward=0.33, Steps=21645, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=737, Total reward=0.4, Steps=21686, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=738, Total reward=0.2, Steps=21707, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=739, Total reward=0.18, Steps=21726, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=740, Total reward=0.45, Steps=21772, Training iteration=36
Policy training> Surrogate loss=0.02480240911245346, KL divergence=0.030956750735640526, Entropy=2.4360175132751465, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.0654054507613182, KL divergence=0.14100205898284912, Entropy=2.4336352348327637, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.09217514842748642, KL divergence=0.18344369530677795, Entropy=2.4314136505126953, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.1106259822845459, KL divergence=0.2400008738040924, Entropy=2.4279932975769043, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.104326032102108, KL divergence=0.252546489238739, Entropy=2.4246039390563965, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.12343604862689972, KL divergence=0.28274834156036377, Entropy=2.4214935302734375, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.11985249817371368, KL divergence=0.27297893166542053, Entropy=2.4187045097351074, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.12089689075946808, KL divergence=0.2787184417247772, Entropy=2.415710926055908, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.12270097434520721, KL divergence=0.29971903562545776, Entropy=2.4130728244781494, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.1274012178182602, KL divergence=0.27779021859169006, Entropy=2.410890579223633, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/37_Step-21772.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 37
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_37.pb
Best checkpoint number: 4, Last checkpoint number: 35
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'34'}
Training> Name=main_level/agent, Worker=0, Episode=741, Total reward=0.24, Steps=21797, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=742, Total reward=9.28, Steps=21843, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=743, Total reward=0.53, Steps=21860, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=744, Total reward=54.09, Steps=21936, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=745, Total reward=36.06, Steps=22006, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=746, Total reward=29.64, Steps=22049, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=747, Total reward=9.95, Steps=22080, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=748, Total reward=0.26, Steps=22107, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=749, Total reward=8.69, Steps=22130, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=750, Total reward=3.41, Steps=22167, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=751, Total reward=24.88, Steps=22221, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=752, Total reward=20.87, Steps=22274, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=753, Total reward=5.52, Steps=22292, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=754, Total reward=0.1, Steps=22303, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=755, Total reward=0.37, Steps=22341, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=756, Total reward=0.32, Steps=22374, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=757, Total reward=0.84, Steps=22459, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=758, Total reward=0.21, Steps=22481, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=759, Total reward=0.49, Steps=22531, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=760, Total reward=0.19, Steps=22551, Training iteration=37
Policy training> Surrogate loss=-0.00496078934520483, KL divergence=0.02702469564974308, Entropy=2.407945156097412, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.03556792065501213, KL divergence=0.14883224666118622, Entropy=2.4044718742370605, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.07761532813310623, KL divergence=0.19334979355335236, Entropy=2.400468111038208, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.08530163764953613, KL divergence=0.21958844363689423, Entropy=2.3953778743743896, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.08068811893463135, KL divergence=0.23537719249725342, Entropy=2.3902587890625, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.08754061907529831, KL divergence=0.24465614557266235, Entropy=2.3853843212127686, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.0802512839436531, KL divergence=0.3007393479347229, Entropy=2.380202054977417, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.10033214092254639, KL divergence=0.2853575050830841, Entropy=2.374485731124878, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.10329966992139816, KL divergence=0.2684673070907593, Entropy=2.369184732437134, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.11129026859998703, KL divergence=0.2941316068172455, Entropy=2.3643622398376465, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/38_Step-22551.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 38
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_38.pb
Best checkpoint number: 4, Last checkpoint number: 36
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'35'}
Training> Name=main_level/agent, Worker=0, Episode=761, Total reward=0.38, Steps=22590, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=762, Total reward=0.29, Steps=22620, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=763, Total reward=0.12, Steps=22633, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=764, Total reward=62.77, Steps=22723, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=765, Total reward=35.22, Steps=22775, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=766, Total reward=18.12, Steps=22807, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=767, Total reward=13.46, Steps=22830, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=768, Total reward=0.25, Steps=22856, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=769, Total reward=6.51, Steps=22877, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=770, Total reward=9.58, Steps=22905, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=771, Total reward=30.23, Steps=22977, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=772, Total reward=9.22, Steps=22998, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=773, Total reward=6.04, Steps=23066, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=774, Total reward=0.13, Steps=23080, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=775, Total reward=0.3, Steps=23111, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=776, Total reward=0.16, Steps=23128, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=777, Total reward=0.17, Steps=23146, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=778, Total reward=0.16, Steps=23163, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=779, Total reward=0.29, Steps=23193, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=780, Total reward=0.25, Steps=23219, Training iteration=38
Policy training> Surrogate loss=0.0007118813809938729, KL divergence=0.016055872663855553, Entropy=2.3593688011169434, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.04099363833665848, KL divergence=0.15299634635448456, Entropy=2.356126308441162, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.09101656079292297, KL divergence=0.21720293164253235, Entropy=2.354283332824707, training epoch=2, learning_rate=0.0003
Policy training> Surrogate loss=-0.07643942534923553, KL divergence=0.2702406048774719, Entropy=2.3525466918945312, training epoch=3, learning_rate=0.0003
Policy training> Surrogate loss=-0.1090453639626503, KL divergence=0.2650497853755951, Entropy=2.3501458168029785, training epoch=4, learning_rate=0.0003
Policy training> Surrogate loss=-0.08375672250986099, KL divergence=0.28434932231903076, Entropy=2.3474156856536865, training epoch=5, learning_rate=0.0003
Policy training> Surrogate loss=-0.09890849888324738, KL divergence=0.30379053950309753, Entropy=2.3446009159088135, training epoch=6, learning_rate=0.0003
Policy training> Surrogate loss=-0.11308588087558746, KL divergence=0.29519402980804443, Entropy=2.3418431282043457, training epoch=7, learning_rate=0.0003
Policy training> Surrogate loss=-0.10634976625442505, KL divergence=0.30188530683517456, Entropy=2.338794231414795, training epoch=8, learning_rate=0.0003
Policy training> Surrogate loss=-0.12138441950082779, KL divergence=0.29341909289360046, Entropy=2.3359551429748535, training epoch=9, learning_rate=0.0003
Checkpoint> Saving in path=['./checkpoint_sagemaker/agent/39_Step-23219.ckpt']
[s3] Successfully uploaded .lock to                      s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.lock.
Uploaded 3 files for checkpoint 39
[s3] Successfully uploaded coach checkpoint to                   s3 bucket bucket with s3 key rl-deepracer-sagemaker/model/.coach_checkpoint.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
saved intermediate frozen graph: rl-deepracer-sagemaker/model/model_39.pb
Best checkpoint number: 4, Last checkpoint number: 37
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
[s3] Successfully downloaded deepracer checkpoint json from                  s3 key rl-deepracer-sagemaker/model/deepracer_checkpoints.json to local checkpoint_sagemaker/agent/deepracer_checkpoints.json.
Deleting the frozen models in s3 for the iterations: {'36'}
Training> Name=main_level/agent, Worker=0, Episode=781, Total reward=1.37, Steps=23254, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=782, Total reward=0.53, Steps=23275, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=783, Total reward=0.11, Steps=23287, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=784, Total reward=44.84, Steps=23353, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=785, Total reward=19.34, Steps=23386, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=786, Total reward=22.57, Steps=23419, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=787, Total reward=5.54, Steps=23461, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=788, Total reward=2.17, Steps=23488, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=789, Total reward=8.89, Steps=23508, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=790, Total reward=8.4, Steps=23530, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=791, Total reward=13.6, Steps=23554, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=792, Total reward=19.53, Steps=23608, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=793, Total reward=3.29, Steps=23630, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=794, Total reward=0.08, Steps=23639, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=795, Total reward=0.13, Steps=23653, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=796, Total reward=0.34, Steps=23688, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=797, Total reward=0.35, Steps=23724, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=798, Total reward=0.19, Steps=23744, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=799, Total reward=0.27, Steps=23772, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=800, Total reward=0.25, Steps=23798, Training iteration=39
Policy training> Surrogate loss=0.02107306569814682, KL divergence=0.026947500184178352, Entropy=2.3334438800811768, training epoch=0, learning_rate=0.0003
Policy training> Surrogate loss=-0.04678638279438019, KL divergence=0.16799089312553406, Entropy=2.3335821628570557, training epoch=1, learning_rate=0.0003
Policy training> Surrogate loss=-0.07152711600065231, KL divergence=0.24717752635478973, Entropy=2.3339223861694336, training epoch=2, learning_rate=0.0003
